{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[],"collapsed_sections":["3N2t882SLUyC"]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":3243177,"sourceType":"datasetVersion","datasetId":1428914},{"sourceId":7650205,"sourceType":"datasetVersion","datasetId":4459722},{"sourceId":8762446,"sourceType":"datasetVersion","datasetId":5264709}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"! pip install tqdm kagglehub python-dotenv opencv-python opencv-contrib-python torch torchvision scikit-learn scikit-image pandas huggingface-hub","metadata":{"id":"6PzM6RMS0B1k","colab":{"base_uri":"https://localhost:8080/"},"outputId":"4ec50836-bba6-4afd-d423-a4d2e3942a96","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport requests\nimport zipfile\nfrom tqdm import tqdm\nimport logging\nfrom dotenv import load_dotenv\nimport shutil\nimport sys\n\n# Load environment variables from .env file\nload_dotenv()\n\n# Set up logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\nlogger.setLevel(logging.INFO)\nlogger.info(\"Logger initialized\")\n\n\ndef download_file(url, filename):\n    logger.info(f\"Downloading {filename} from {url}\")\n    response = requests.get(url, stream=True)\n    total_size = int(response.headers.get('content-length', 0))\n    block_size = 1024\n    t = tqdm(total=total_size, unit='B', unit_scale=True)\n    with open(filename, 'wb') as f:\n        for data in response.iter_content(block_size):\n            t.update(len(data))\n            f.write(data)\n    t.close()\n\ndef unzip_file(zip_path, extract_to):\n    logger.info(f\"Unzipping {zip_path} to {extract_to}\")\n    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n        zip_ref.extractall(extract_to)\n    logger.info(f\"Unzipped {zip_path} to {extract_to}\")\n\ndef fetch_bus_images_dataset(output_path):\n    # URL of the BUSI dataset\n    url = 'https://scholar.cu.edu.eg/Dataset_BUSI.zip'\n    filename = os.path.join(output_path, 'BUSI.zip')\n    download_file(url, filename)\n    unzip_file(filename, output_path)\n    os.remove(filename)\n\ndef fetch_bus_dataset(output_path):\n    url = 'https://data.mendeley.com/public-files/datasets/wmy84gzngw/files/b63daee9-78de-4122-8475-9b3aa22ffd64/file_downloaded'\n    filename = os.path.join(output_path, 'BUS.zip')\n    headers = {'User-Agent': 'Mozilla/5.0'}\n    response = requests.get(url, headers=headers, stream=True)\n    total_size = int(response.headers.get('content-length', 0))\n    block_size = 1024\n    t = tqdm(total=total_size, unit='B', unit_scale=True)\n    with open(filename, 'wb') as f:\n        for data in response.iter_content(block_size):\n            t.update(len(data))\n            f.write(data)\n    t.close()\n    unzip_file(filename, output_path)\n    os.remove(filename)\n\n\ndef fetch_vindr_metadata(output_path):\n    import kagglehub\n    # os.environ[\"KAGGLE_CONFIG\"] = os.path.join(os.path.dirname(__file__), \"./kaggle/kaggle.json\")\n    # Download latest version\n    path = kagglehub.dataset_download(\"truthisneverlinear/vindr-mammo-annotations\")\n    logger.info(f\"Downloaded dataset to {path}\")\n    move_dataset(path, output_path)\n\n\ndef move_dataset(path, output_path):\n    # Move the dataset to the output path\n    os.makedirs(output_path, exist_ok=True)\n    for item in os.listdir(path):\n        src = os.path.join(path, item)\n        dst = os.path.join(output_path, item)\n        if os.path.exists(dst):\n            if os.path.isdir(dst):\n                shutil.rmtree(dst)\n            else:\n                os.remove(dst)\n        shutil.move(src, dst)\n    os.rmdir(path)\n    logger.info(f\"Moved dataset to {output_path}\")\n\ndef fetch_vindr_dataset(output_path):\n    import kagglehub\n    # os.environ[\"KAGGLE_CONFIG\"] = os.path.join(os.path.dirname(__file__), \"./kaggle/kaggle.json\")\n    # Download latest version\n    path = kagglehub.dataset_download(\"shantanughosh/vindr-mammogram-dataset-dicom-to-png\")\n    logger.info(f\"Downloaded dataset to {path}\")\n    move_dataset(path, output_path)\n\n\ndef main():\n    output_path = 'data'\n    os.makedirs(output_path, exist_ok=True)\n\n    # fetch_bus_dataset(output_path)\n    # fetch_bus_images_dataset(output_path)\n    # fetch_vindr_dataset(output_path)\n    # fetch_vindr_metadata(output_path=\"data\")\n\n\nif __name__ == \"__main__\":\n    logger.info(\"Script started\")\n    main()\n    logger.info(\"Script finished\")\n    logger.info(\"All datasets fetched successfully\")\n","metadata":{"id":"WeQ1ERAB0MwN","colab":{"base_uri":"https://localhost:8080/"},"outputId":"1e03f5a8-f20d-4e33-899c-db4547714a0e","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import shutil\n\nshutil.copytree('/kaggle/working/data/images_png', './data/mammo/images')\n\n!rm -rf /kaggle/working/data/images_png","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"9l6vPK9zL8R3","outputId":"d9a4a75e-7681-45ee-e877-91fa9559c423","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# prompt: check the content of a directory code and show all format\n\nimport os\n\ndef list_files_with_formats(directory):\n  \"\"\"Lists all files in a directory, showing their formats.\"\"\"\n  try:\n    for filename in os.listdir(directory):\n      filepath = os.path.join(directory, filename)\n      if os.path.isfile(filepath):\n        name, ext = os.path.splitext(filename)\n        print(f\"File: {filename}, Format: {ext}\")\n      elif os.path.isdir(filepath):\n        print(f\"Directory: {filename}\")\n        list_files_with_formats(filepath) # Recursive call for subdirectories\n  except FileNotFoundError:\n      print(f\"Directory '{directory}' not found.\")\n  except Exception as e:\n      print(f\"An error occurred: {e}\")\n\n\n# Example usage:\nlist_files_with_formats(\"./data/mammo/images\") # Replace with the actual path\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"h9d5YzUqa6uc","outputId":"d1f7f144-7f76-4819-ff6f-f2f8d5df1a74","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\n\nshutil.copy('/kaggle/input/vindr-mammo-annotations/breast-level_annotations.csv', './data/mammo/breast-level_annotations.csv')\n\nshutil.copy('/kaggle/input/vindr-mammo-annotations/finding_annotations.csv', './data/mammo/finding_annotations.csv')\n\nshutil.copy('/kaggle/input/vindr-mammo-annotations/metadata.csv', './data/mammo/metadata.csv')\n\nshutil.copy('/kaggle/input/vindr-mammo-annotations/SHA256SUMS.txt', './data/mammo/SHA256SUMS.txt')\n","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"1jcOf07CesOQ","outputId":"626edf5b-40a9-49bf-a518-dda94f6a71e8","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# prompt: code to move the content of ./data/Dataset_BUSI_with_GT/ into ./data/ultrasound/masks and ./data/ultrasound/images, but checking each directory within the main directory for the appropriate mask images to move into ./data/ultrasound/masks and normal images to move into ./data/ultrasound/images. Mask images contain \"_mask\"; make sure it's just moving the necessary files from the dataset Busi into ultrasound. make sure the sub-directory under the /data/Dataset_BUSI_with_GT/  is used as same subdirectory under the ultrasound/images and ultrasound/masks\n\nimport os\nimport shutil\n\ndef organize_images_and_masks(source_dir, images_dir, masks_dir):\n    \"\"\"Recursively move .png images and masks to separate directories.\"\"\"\n    os.makedirs(images_dir, exist_ok=True)\n    os.makedirs(masks_dir, exist_ok=True)\n\n    for root, _, files in os.walk(source_dir):\n        for filename in files:\n            if filename.endswith('.png'):\n                src_path = os.path.join(root, filename)\n                rel_path = os.path.relpath(root, source_dir)\n\n                # Create corresponding subdirectories in images_dir and masks_dir\n                image_subdir = os.path.join(images_dir, rel_path)\n                mask_subdir = os.path.join(masks_dir, rel_path)\n                os.makedirs(image_subdir, exist_ok=True)\n                os.makedirs(mask_subdir, exist_ok=True)\n\n                if \"_mask\" in filename:\n                    dst_path = os.path.join(mask_subdir, filename)\n                else:\n                    dst_path = os.path.join(image_subdir, filename)\n\n                shutil.move(src_path, dst_path)\n\nif __name__ == \"__main__\":\n    source_dir = \"./data/Dataset_BUSI_with_GT\"\n    images_dir = './data/ultrasound/images'\n    masks_dir = './data/ultrasound/masks'\n    organize_images_and_masks(source_dir, images_dir, masks_dir)","metadata":{"id":"lbYLUmTGyaYR","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!rm -rf /kaggle/working/data/Dataset_BUSI_with_GT","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport shutil\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\n# --- Configuration ---\n# Set the base path for your downloaded BUSI dataset\n# Assuming the structure is ./data/ultrasound/images/... and ./data/ultrasound/masks/...\nBUSI_ORIGINAL_PATH = './data/ultrasound' # Adjust this path if needed\nBUSI_IMAGES_PATH = os.path.join(BUSI_ORIGINAL_PATH, 'images')\nBUSI_MASKS_PATH = os.path.join(BUSI_ORIGINAL_PATH, 'masks')\n\n\n# Set the base path for the new organized dataset structure\nORGANIZED_DATASET_BASE_PATH = './mmibc/busi' # Specific path for BUSI\nORGANIZED_METADATA_FILE = os.path.join(ORGANIZED_DATASET_BASE_PATH, 'busi_metadata.csv') # Output metadata file\n\n# Define split ratios (train, validation, test) - should sum to 1.0\nTRAIN_RATIO = 0.8\nVAL_RATIO = 0.1\nTEST_RATIO = 0.1\n\n# --- Function to Get Image Labels and Paths for BUSI ---\ndef get_image_labels_and_paths_busi(images_base_path, masks_base_path):\n    \"\"\"\n    Reads BUSI dataset structure (assuming images/masks subfolders with\n    benign, malignant, normal subfolders inside) and assigns binary labels\n    (0 for benign/normal, 1 for malignant).\n    Correctly finds mask files based on '_mask' naming convention.\n    Returns a list of dictionaries: [{'image_id': '...', 'label': 0/1,\n                                       'image_file_path': '...', 'mask_file_path': '...',\n                                       'original_class': '...'}]\n    \"\"\"\n    print(\"Processing BUSI dataset structure...\")\n\n    image_info_list = [] # List to store info for each image\n\n    # Define the original class directories and their target binary labels\n    class_directories = {\n        'benign': 0,    # Map benign to binary label 0\n        'normal': 0,    # Map normal to binary label 0\n        'malignant': 1  # Map malignant to binary label 1\n    }\n\n    for class_name, binary_label in class_directories.items():\n        class_images_dir_path = os.path.join(images_base_path, class_name)\n        class_masks_dir_path = os.path.join(masks_base_path, class_name)\n\n\n        if not os.path.exists(class_images_dir_path):\n            print(f\"Warning: Image directory not found: {class_images_dir_path}. Skipping this class.\")\n            continue\n        # Note: We don't skip the class if mask directory is missing, just note it.\n        if not os.path.exists(class_masks_dir_path):\n             print(f\"Warning: Mask directory not found: {class_masks_dir_path}. Segmentation masks might be missing for this class.\")\n\n\n        print(f\"Processing class: {class_name} (Binary Label: {binary_label})\")\n\n        # List files in the class image directory\n        try:\n            image_files = [f for f in os.listdir(class_images_dir_path) if not f.startswith('.') and os.path.isfile(os.path.join(class_images_dir_path, f))]\n        except Exception as e:\n            print(f\"Error listing files in {class_images_dir_path}: {e}\")\n            continue\n\n        for file_name in image_files:\n            image_file_path = os.path.join(class_images_dir_path, file_name)\n\n            # Construct the expected mask filename by appending '_mask' before the extension\n            name, ext = os.path.splitext(file_name)\n            mask_file_name = f\"{name}_mask{ext}\"\n            mask_file_path = os.path.join(class_masks_dir_path, mask_file_name)\n\n            # Check if the constructed mask file path actually exists\n            if not os.path.exists(mask_file_path):\n                 # Handle cases like 'benign (4)_mask_1.png' if they exist alongside 'benign (4)_mask.png'\n                 # This might require more sophisticated logic if there are multiple masks per image.\n                 # For now, we'll just check for the primary '_mask' version.\n                 # If the primary mask isn't found, check for '_mask_1', '_mask_2', etc.\n                 # This is a common pattern in BUSI for multiple masks per image.\n                 # Let's refine this to find *all* masks associated with an image ID.\n\n                 associated_masks = []\n                 if os.path.exists(class_masks_dir_path):\n                     mask_files_in_dir = os.listdir(class_masks_dir_path)\n                     # Find all files in the mask directory that start with the image name\n                     # This is a more robust way to find associated masks\n                     base_name = os.path.splitext(file_name)[0]\n                     associated_mask_files = [\n                         f for f in mask_files_in_dir\n                         if f.startswith(base_name) and '_mask' in f and os.path.isfile(os.path.join(class_masks_dir_path, f))\n                     ]\n                     associated_masks = [os.path.join(class_masks_dir_path, f) for f in associated_mask_files]\n\n\n                 if not associated_masks:\n                     print(f\"Warning: No mask file found for image {file_name} in {class_masks_dir_path}. Mask path will be None.\")\n                     mask_file_path_to_record = None # No mask found\n                 elif len(associated_masks) > 1:\n                      print(f\"Warning: Found multiple masks for image {file_name}: {associated_masks}. Recording the first one found.\")\n                      mask_file_path_to_record = associated_masks[0] # Or decide how to handle multiple masks\n                 else:\n                      mask_file_path_to_record = associated_masks[0] # Only one mask found\n\n            else:\n                 # The primary '_mask' file was found\n                 mask_file_path_to_record = mask_file_path\n\n\n            # Use file_name as a simple image_id for BUSI\n            image_id = file_name\n\n            image_info_list.append({\n                'image_id': image_id,\n                'label': binary_label,\n                'image_file_path': image_file_path,\n                'mask_file_path': mask_file_path_to_record, # Include the found mask path (or None)\n                'original_class': class_name # Keep original class name for metadata\n            })\n\n    print(f\"Finished processing BUSI structure. Found {len(image_info_list)} relevant images with binary labels.\")\n    return image_info_list\n\n# --- Function to Split Data ---\ndef split_busi_data(image_info_list, train_ratio, val_ratio, test_ratio):\n    \"\"\"\n    Splits the image info list into train, validation, and test sets,\n    stratifying by the binary label.\n    Returns a dictionary mapping split names to lists of dictionaries.\n    \"\"\"\n    print(\"Splitting BUSI data...\")\n\n    if not image_info_list:\n        print(\"No image info found to split.\")\n        return {'train': [], 'validation': [], 'test': []}\n\n    # Extract binary labels for splitting\n    all_labels = [item['label'] for item in image_info_list]\n\n    # Ensure there are enough samples and classes for stratification\n    if len(set(all_labels)) < 2 or len(all_labels) < 2:\n         print(\"Warning: Not enough samples or classes for stratification. Splitting without stratify.\")\n         train_info, temp_info = train_test_split(\n            image_info_list, test_size=(val_ratio + test_ratio), random_state=42\n        )\n    else:\n        # Perform the split using the list of dictionaries directly\n        train_info, temp_info = train_test_split(\n            image_info_list, test_size=(val_ratio + test_ratio), stratify=all_labels, random_state=42\n        )\n\n    # Calculate test_size for the second split relative to the temporary set\n    test_size_temp = val_ratio + test_ratio\n    test_size_final = test_ratio / test_size_temp if test_size_temp > 0 else 0\n\n    # Extract labels from the temporary split for stratification\n    temp_labels = [item['label'] for item in temp_info]\n\n    if test_size_final > 0 and (len(set(temp_labels)) < 2 or len(temp_labels) < 2):\n         print(\"Warning: Not enough samples or classes in temporary split for second stratification. Splitting without stratify.\")\n         val_info, test_info = train_test_split(\n            temp_info, test_size=test_size_final, random_state=42\n        )\n    else:\n        val_info, test_info = train_test_split(\n            temp_info, test_size=test_size_final, stratify=temp_labels, random_state=42\n        )\n\n    splits = {\n        'train': train_info,\n        'validation': val_info,\n        'test': test_info\n    }\n\n    print(f\"Splitting complete. Train: {len(splits['train'])}, Val: {len(splits['validation'])}, Test: {len(splits['test'])}\")\n    return splits\n\n# --- Function to Organize Split Files and Generate Metadata ---\ndef organize_split_files_and_generate_metadata_busi(split_data, organized_base_path, metadata_output_path):\n    \"\"\"\n    Organizes the split data (list of dictionaries) into the target directory structure\n    and generates a metadata CSV file for the organized data for BUSI.\n    Copies both images and masks.\n    \"\"\"\n    print(f\"Organizing split files into: {organized_base_path}\")\n\n    all_organized_metadata = []\n\n    # Create base organized directories\n    organized_images_base = os.path.join(organized_base_path, 'images')\n    organized_masks_base = os.path.join(organized_base_path, 'masks')\n    os.makedirs(organized_images_base, exist_ok=True)\n    os.makedirs(organized_masks_base, exist_ok=True)\n\n\n    for split_name, data_list in split_data.items():\n        # Create split directories within images and masks\n        split_images_dir = os.path.join(organized_images_base, split_name)\n        split_masks_dir = os.path.join(organized_masks_base, split_name)\n\n        # Create class directories within each split directory\n        benign_images_dir = os.path.join(split_images_dir, 'benign')\n        malignant_images_dir = os.path.join(split_images_dir, 'malignant')\n        benign_masks_dir = os.path.join(split_masks_dir, 'benign')\n        malignant_masks_dir = os.path.join(split_masks_dir, 'malignant')\n\n\n        os.makedirs(benign_images_dir, exist_ok=True)\n        os.makedirs(malignant_images_dir, exist_ok=True)\n        os.makedirs(benign_masks_dir, exist_ok=True)\n        os.makedirs(malignant_masks_dir, exist_ok=True)\n\n\n        print(f\"Copying files for {split_name}...\")\n        for image_info in data_list:\n            original_image_file_path = image_info['image_file_path']\n            original_mask_file_path = image_info['mask_file_path'] # This is the path found in get_image_labels_and_paths_busi\n            label = image_info['label']\n            image_id = image_info['image_id'] # Use original file name as image_id\n            original_class = image_info['original_class'] # Keep original class name\n\n            # Determine the target directory for image and mask\n            target_images_dir = benign_images_dir if label == 0 else malignant_images_dir\n            target_masks_dir = benign_masks_dir if label == 0 else malignant_masks_dir\n\n            # Use original image_id as the new file name for the image\n            new_image_file_name = image_id\n            organized_image_file_path = os.path.join(target_images_dir, new_image_file_name)\n\n            # For the mask, use the original mask file name if available, otherwise use the image_id + _mask\n            if original_mask_file_path:\n                 new_mask_file_name = os.path.basename(original_mask_file_path)\n            else:\n                 # If no mask was found, we won't copy one, but we could define a potential name\n                 # for consistency in metadata, though it won't exist on disk.\n                 # Let's just record None for the organized path if original was None.\n                 new_mask_file_name = None\n\n            organized_mask_file_path = os.path.join(target_masks_dir, new_mask_file_name) if new_mask_file_name else None\n\n\n            try:\n                # Copy the image\n                shutil.copy(original_image_file_path, organized_image_file_path)\n\n                # Copy the mask if it exists\n                if original_mask_file_path and os.path.exists(original_mask_file_path):\n                     shutil.copy(original_mask_file_path, organized_mask_file_path)\n                else:\n                     organized_mask_file_path = None # Ensure organized path is None if mask wasn't copied\n\n\n                # Add information to the metadata list\n                metadata_entry = {\n                    'image_id': image_id,\n                    'label': label, # Binary label (0 or 1)\n                    'original_class': original_class, # Original class (benign, malignant, normal)\n                    'split': split_name, # This is our new split (train, val, test)\n                    'organized_image_file_path': organized_image_file_path,\n                    'original_image_file_path': original_image_file_path,\n                    'organized_mask_file_path': organized_mask_file_path, # Path to the copied mask (or None)\n                    'original_mask_file_path': original_mask_file_path # Path to the original mask (or None)\n                }\n                all_organized_metadata.append(metadata_entry)\n\n            except FileNotFoundError:\n                 print(f\"Error: Source file not found during copy: {original_image_file_path} or {original_mask_file_path}\")\n            except Exception as e:\n                 print(f\"Error copying file {original_image_file_path} or its mask: {e}\")\n\n\n        print(f\"Finished copying for {split_name}. Total files: {len(data_list)}\")\n\n    # Generate the metadata CSV\n    if all_organized_metadata:\n        metadata_df = pd.DataFrame(all_organized_metadata)\n        # Ensure the output directory exists\n        os.makedirs(os.path.dirname(metadata_output_path), exist_ok=True)\n        metadata_df.to_csv(metadata_output_path, index=False)\n        print(f\"\\nGenerated organized dataset metadata file: {metadata_output_path}\")\n    else:\n        print(\"\\nNo metadata entries to write. Metadata file not generated.\")\n\n\n# --- Main Execution ---\nif __name__ == \"__main__\":\n    # 1. Process BUSI dataset structure and Get Image Info\n    busi_image_info_list = get_image_labels_and_paths_busi(\n        BUSI_IMAGES_PATH, # Pass the images subfolder path\n        BUSI_MASKS_PATH   # Pass the masks subfolder path\n    )\n\n    if busi_image_info_list:\n        # 2. Split Data\n        busi_splits = split_busi_data(\n            busi_image_info_list,\n            TRAIN_RATIO,\n            VAL_RATIO,\n            TEST_RATIO\n        )\n\n        # 3. Organize Split Files and Generate Metadata\n        organize_split_files_and_generate_metadata_busi(\n            busi_splits,\n            ORGANIZED_DATASET_BASE_PATH,\n            ORGANIZED_METADATA_FILE\n        )\n        print(\"\\nBUSI data preparation complete.\")\n    else:\n        print(\"\\nBUSI data preparation failed due to issues processing dataset structure or finding images.\")\n\n    # Note: This script only handles BUSI.\n    # You would run the VinDr-Mammo script separately,\n    # handle KAU-BCMD, and then use a separate script to combine/upload to Hugging Face.","metadata":{"id":"Z3p5hllwgaUa","colab":{"base_uri":"https://localhost:8080/"},"outputId":"6cdcb002-20ae-4107-a911-155670387242","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!rm -rf /kaggle/working/data/originals","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport shutil\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\n# --- Configuration ---\n# Set the base path for your downloaded VinDr-Mammo dataset\nVINDR_MAMMO_ORIGINAL_PATH = './data/mammo' # Adjust this path if needed\n\n# Paths to the VinDr-Mammo metadata files\nBREAST_LEVEL_METADATA_PATH = os.path.join(VINDR_MAMMO_ORIGINAL_PATH, 'breast-level_annotations.csv')\nFINDING_METADATA_PATH = os.path.join(VINDR_MAMMO_ORIGINAL_PATH, 'finding_annotations.csv')\n# METADATA_CSV_PATH = os.path.join(VINDR_MAMMO_ORIGINAL_PATH, 'metadata.csv') # Optional: for image details\n\n# Set the base path for the new organized dataset structure\nORGANIZED_DATASET_BASE_PATH = './mmibc/vindr_mammo' # Specific path for VinDr-Mammo\nORGANIZED_METADATA_FILE = os.path.join(ORGANIZED_DATASET_BASE_PATH, 'vindr_mammo_metadata.csv') # Output metadata file\n\n# Define split ratios for splitting the *original training data* into our train/validation sets\n# The original 'test' split will be used as our test set.\nTRAIN_RATIO_ORIGINAL_TRAINING = 0.8 # Ratio of original training data for our training set\nVAL_RATIO_ORIGINAL_TRAINING = 0.2  # Ratio of original training data for our validation set\n# Note: Original test data will be used entirely for our test set.\n\n# --- Function to Process Metadata and Get Image Labels ---\ndef get_image_labels_and_paths(original_data_path, breast_metadata_path, finding_metadata_path):\n    \"\"\"\n    Reads VinDr-Mammo metadata to determine binary labels (benign/malignant)\n    and maps image IDs to file paths. Includes original split and study_id.\n    Returns a list of dictionaries: [{'image_id': '...', 'label': 0/1, 'original_split': '...', 'file_path': '...', 'study_id': '...'}]\n    \"\"\"\n    print(\"Processing VinDr-Mammo metadata...\")\n\n    image_info_list = [] # List to store info for each image\n\n    # Load metadata files\n    finding_df = None\n    if os.path.exists(finding_metadata_path):\n        finding_df = pd.read_csv(finding_metadata_path)\n        # Map finding BI-RADS to binary malignancy (4, 5, 6 are malignant)\n        finding_df['is_malignant_finding'] = finding_df['finding_birads'].isin(['BI-RADS 4', 'BI-RADS 5', 'BI-RADS 6']).astype(int)\n    else:\n        print(f\"Warning: Finding metadata file not found at {finding_metadata_path}. Malignant cases might be missed.\")\n\n    breast_df = None\n    if os.path.exists(breast_metadata_path):\n        breast_df = pd.read_csv(breast_metadata_path)\n    else:\n        print(f\"Warning: Breast-level metadata file not found at {breast_metadata_path}. Benign/normal cases might be missed.\")\n\n    # Combine relevant info from both dataframes\n    if finding_df is not None and breast_df is not None:\n        # Use breast_df as the base, as it lists all images (including normals)\n        # Merge finding info onto breast info\n        merged_df = pd.merge(breast_df, finding_df[['image_id', 'is_malignant_finding']], on='image_id', how='left')\n        # Fill NaN malignant findings with 0 (meaning no malignant finding in this image)\n        merged_df['is_malignant_finding'] = merged_df['is_malignant_finding'].fillna(0)\n\n        # Determine final binary label (1 if any malignant finding, 0 otherwise)\n        merged_df['final_label'] = merged_df['is_malignant_finding'].clip(upper=1) # Ensure label is 0 or 1\n\n        # Now, iterate through the merged dataframe to collect image info\n        for index, row in merged_df.iterrows():\n            image_id = row['image_id']\n            study_id = row['study_id']\n            original_split = row['split']\n            label = int(row['final_label']) # Ensure label is integer 0 or 1\n\n            # Construct the full path to the image file\n            # Assuming .png extension based on SHA256SUMS, adjust if necessary (e.g., .dicom)\n            image_file_path = os.path.join(original_data_path, 'images', study_id, f'{image_id}.png')\n\n\n            if os.path.exists(image_file_path):\n                image_info_list.append({\n                    'image_id': image_id,\n                    'study_id': study_id,\n                    'label': label,\n                    'original_split': original_split,\n                    'file_path': image_file_path,\n                    # Add other relevant metadata if needed, e.g., 'breast_birads', 'finding_categories'\n                    'breast_birads': row.get('breast_birads'), # Use .get to avoid error if column is missing\n                    'breast_density': row.get('breast_density'),\n                    # Note: finding_categories is a list in CSV, might need careful handling\n                })\n            else:\n                 print(f\"Warning: Image file not found for image {image_id} at {image_file_path}. Skipping.\")\n\n    elif breast_df is not None:\n         print(\"Proceeding with breast-level metadata only. Malignant cases from findings might be missed.\")\n         for index, row in breast_df.iterrows():\n            image_id = row['image_id']\n            study_id = row['study_id']\n            original_split = row['split']\n            # Assume benign/normal if only breast-level metadata is available and BI-RADS is 1, 2, or 3\n            label = 0 if row['breast_birads'] in ['BI-RADS 1', 'BI-RADS 2', 'BI-RADS 3'] else -1 # Use -1 for uncertain\n\n            if label != -1:\n                 image_file_path = os.path.join(original_data_path, 'images', study_id, f'{image_id}.png')\n                 if os.path.exists(image_file_path):\n                     image_info_list.append({\n                         'image_id': image_id,\n                         'study_id': study_id,\n                         'label': label,\n                         'original_split': original_split,\n                         'file_path': image_file_path,\n                         'breast_birads': row.get('breast_birads'),\n                         'breast_density': row.get('breast_density'),\n                     })\n                 else:\n                     print(f\"Warning: Image file not found for image {image_id} at {image_file_path}. Skipping.\")\n            else:\n                 print(f\"Warning: Uncertain label for image {image_id} based on breast-level BI-RADS {row['breast_birads']}. Skipping.\")\n\n    elif finding_df is not None:\n         print(\"Proceeding with finding-level metadata only. Benign/normal cases might be missed.\")\n         # In this case, we only have info about images with findings\n         for index, row in finding_df.iterrows():\n             image_id = row['image_id']\n             study_id = row['study_id']\n             original_split = row['split']\n             label = int(row['is_malignant_finding']) # Label is 1 if malignant finding, 0 otherwise\n\n             image_file_path = os.path.join(original_data_path, 'images', study_id, f'{image_id}.png')\n             if os.path.exists(image_file_path):\n                 image_info_list.append({\n                     'image_id': image_id,\n                     'study_id': study_id,\n                     'label': label,\n                     'original_split': original_split,\n                     'file_path': image_file_path,\n                     # Breast-level info is not available here\n                 })\n             else:\n                 print(f\"Warning: Image file not found for image {image_id} at {image_file_path}. Skipping.\")\n\n\n    else:\n        print(\"Error: Neither breast-level nor finding-level metadata files were found.\")\n\n\n    print(f\"Finished metadata processing. Found {len(image_info_list)} relevant images with labels.\")\n    return image_info_list\n\n# --- Function to Split Data Based on Original Split and Ratios ---\ndef split_vindr_mammo_data(image_info_list, train_ratio_orig_train, val_ratio_orig_train):\n    \"\"\"\n    Splits the image info list into train, validation, and test sets,\n    respecting the original 'split' column and applying ratios to the original training data.\n    Returns a dictionary mapping split names to lists of dictionaries.\n    \"\"\"\n    print(\"Splitting VinDr-Mammo data...\")\n\n    # Separate based on original split\n    original_training_data = [item for item in image_info_list if item['original_split'] == 'training']\n    original_test_data = [item for item in image_info_list if item['original_split'] == 'test']\n\n    print(f\"Original training data count: {len(original_training_data)}\")\n    print(f\"Original test data count: {len(original_test_data)}\")\n\n    train_split_info = []\n    val_split_info = []\n\n    # Split original training data into our train and validation sets\n    if original_training_data:\n        # Extract file paths and labels for splitting\n        orig_train_files = [item['file_path'] for item in original_training_data]\n        orig_train_labels = [item['label'] for item in original_training_data]\n\n        # Perform the split\n        # Ensure there are enough samples for stratification and splitting\n        if len(set(orig_train_labels)) < 2 or len(orig_train_labels) < 2:\n             print(\"Warning: Not enough samples or classes in original training data for stratification. Splitting without stratify.\")\n             files_train, files_val, labels_train, labels_val = train_test_split(\n                orig_train_files, orig_train_labels,\n                test_size=(val_ratio_orig_train / (train_ratio_orig_train + val_ratio_orig_train)),\n                random_state=42\n            )\n        else:\n            files_train, files_val, labels_train, labels_val = train_test_split(\n                orig_train_files, orig_train_labels,\n                test_size=(val_ratio_orig_train / (train_ratio_orig_train + val_ratio_orig_train)), # Calculate test_size relative to the subset\n                stratify=orig_train_labels,\n                random_state=42\n            )\n\n        # Reconstruct the list of dictionaries for the splits\n        # Need to look up the original info based on file_path\n        original_info_map = {item['file_path']: item for item in original_training_data}\n\n        train_split_info = [original_info_map[f] for f in files_train]\n        val_split_info = [original_info_map[f] for f in files_val]\n\n    else:\n        print(\"Warning: No original training data found. Train and validation sets will be empty.\")\n\n\n    # The original test data becomes our test set\n    # We only need file_path and label for the organized structure\n    test_split_info = [{'file_path': item['file_path'], 'label': item['label'], 'image_id': item['image_id'], 'study_id': item['study_id']} for item in original_test_data]\n\n\n    splits = {\n        'train': train_split_info,\n        'validation': val_split_info,\n        'test': test_split_info\n    }\n\n    print(f\"Splitting complete. Train: {len(splits['train'])}, Val: {len(splits['validation'])}, Test: {len(splits['test'])}\")\n    return splits\n\n# --- Function to Organize Split Files and Generate Metadata ---\ndef organize_split_files_and_generate_metadata(split_data, organized_base_path, metadata_output_path):\n    \"\"\"\n    Organizes the split data (list of dictionaries) into the target directory structure\n    and generates a metadata CSV file for the organized data.\n    \"\"\"\n    print(f\"Organizing split files into: {organized_base_path}\")\n\n    all_organized_metadata = []\n\n    for split_name, data_list in split_data.items():\n        split_dir = os.path.join(organized_base_path, split_name)\n        benign_split_dir = os.path.join(split_dir, 'benign')\n        malignant_split_dir = os.path.join(split_dir, 'malignant')\n\n        os.makedirs(benign_split_dir, exist_ok=True)\n        os.makedirs(malignant_split_dir, exist_ok=True)\n\n        print(f\"Copying files for {split_name}...\")\n        for image_info in data_list:\n            original_file_path = image_info['file_path']\n            label = image_info['label']\n            image_id = image_info['image_id']\n            study_id = image_info['study_id'] # Keep study_id for metadata\n\n            # Determine the target directory\n            target_dir = benign_split_dir if label == 0 else malignant_split_dir\n            # Use original image_id as the new file name\n            new_file_name = f'{image_id}.png' # Assuming .png, adjust if needed\n            organized_file_path = os.path.join(target_dir, new_file_name)\n\n            try:\n                shutil.copy(original_file_path, organized_file_path)\n\n                # Add information to the metadata list\n                metadata_entry = {\n                    'image_id': image_id,\n                    'study_id': study_id,\n                    'label': label,\n                    'split': split_name, # This is our new split (train, val, test)\n                    'organized_file_path': organized_file_path,\n                    'original_file_path': original_file_path,\n                    # Include other relevant info from original metadata if available\n                    'breast_birads': image_info.get('breast_birads'),\n                    'breast_density': image_info.get('breast_density'),\n                    # Add path to segmentation mask if available and organized separately\n                    # 'segmentation_mask_path': 'path/to/mask.png' # TODO: Add logic for mask paths\n                }\n                all_organized_metadata.append(metadata_entry)\n\n            except FileNotFoundError:\n                 print(f\"Error: Source file not found during copy: {original_file_path}\")\n            except Exception as e:\n                 print(f\"Error copying file {original_file_path} to {organized_file_path}: {e}\")\n\n\n        print(f\"Finished copying for {split_name}. Total files: {len(data_list)}\")\n\n    # Generate the metadata CSV\n    if all_organized_metadata:\n        metadata_df = pd.DataFrame(all_organized_metadata)\n        # Ensure the output directory exists\n        os.makedirs(os.path.dirname(metadata_output_path), exist_ok=True)\n        metadata_df.to_csv(metadata_output_path, index=False)\n        print(f\"\\nGenerated organized dataset metadata file: {metadata_output_path}\")\n    else:\n        print(\"\\nNo metadata entries to write. Metadata file not generated.\")\n\n\n# --- Main Execution ---\nif __name__ == \"__main__\":\n    # 1. Process Metadata and Get Image Info\n    vindr_mammo_image_info_list = get_image_labels_and_paths(\n        VINDR_MAMMO_ORIGINAL_PATH,\n        BREAST_LEVEL_METADATA_PATH,\n        FINDING_METADATA_PATH\n    )\n\n    if vindr_mammo_image_info_list:\n        # 2. Split Data\n        vindr_mammo_splits = split_vindr_mammo_data(\n            vindr_mammo_image_info_list,\n            TRAIN_RATIO_ORIGINAL_TRAINING,\n            VAL_RATIO_ORIGINAL_TRAINING\n        )\n\n        # 3. Organize Split Files and Generate Metadata\n        organize_split_files_and_generate_metadata(\n            vindr_mammo_splits,\n            ORGANIZED_DATASET_BASE_PATH,\n            ORGANIZED_METADATA_FILE\n        )\n        print(\"\\nVinDr-Mammo data preparation complete.\")\n    else:\n        print(\"\\nVinDr-Mammo data preparation failed due to issues processing metadata or finding images.\")\n","metadata":{"id":"a6Otco1jtfdn","colab":{"base_uri":"https://localhost:8080/"},"outputId":"0c57fc7d-1e3d-43fd-dc09-8e325585b7e6","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!rm -rf /kaggle/working/data/mammo/images","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Huggingface Upload","metadata":{"id":"3N2t882SLUyC"}},{"cell_type":"markdown","source":"\n\n## Multimodal Breast Cancer Imaging Dataset\n\nThis dataset combines publicly available mammography and ultrasound imaging data for the development and evaluation of multimodal deep learning models for breast cancer diagnosis, with a focus on interpretability and applicability in resource-limited settings.\n\n## Dataset Sources\n\nThis dataset is compiled from the following publicly available sources:\n\n* **VinDr-Mammo:** A large-scale benchmark dataset for computer-aided detection and diagnosis in full-field digital mammography.\n    * **Source:** [https://doi.org/10.13026/br2v-7517](https://doi.org/10.13026/br2v-7517)\n    * **Reference:** Pham, H. H., Nguyen-Trung, H., & Nguyen, H. Q. (2022). VinDr-Mammo: A large-scale benchmark dataset for computer-aided detection and diagnosis in full-field digital mammography (version 1.0.0). *PhysioNet*.\n\n* **BUSI (Breast Ultrasound Images) Dataset:** A well-curated collection of breast ultrasound images.\n    * **Source:** [https://doi.org/10.1016/j.dib.2019.104863](https://doi.org/10.1016/j.dib.2019.104863)\n    * **Reference:** Al-Dhabyani, W., Gomaa, M., Khaled, H., & Fahmy, A. (2020). Dataset of breast ultrasound images. *Data in Brief*, 28, 104863.\n\n* **KAU-BCMD (King Abdulaziz University Breast Cancer Mammogram Dataset):** Includes paired mammography and ultrasound images for a subset of cases, valuable for multimodal evaluation.\n    * **Source:** [https://doi.org/10.3390/data6110111](https://doi.3390/data6110111)\n    * **Reference:** Alsolami, A. S., Shalash, W., Alsaggaf, W., Ashoor, S., Refaat, H., & Elmogy, M. (2021). King Abdulaziz University Breast Cancer Mammogram Dataset (KAU-BCMD). *Data*, 6(11), 111.\n\n## Dataset Structure\n\nThe dataset is organized into folders based on modality and data split. The proposed structure is as follows:\n```\n.mmibc/\n├── mammo/\n│   ├── train/\n│   │   ├── benign/\n│   │   └── malignant/\n│   ├── validation/\n│   │   ├── benign/\n│   │   └── malignant/\n│   └── test/\n│       ├── benign/\n│       └── malignant/\n├── busi/\n│   ├──images/\n│   │   |── train/\n│   │   │       ├── benign/\n│   │   │       └── malignant/\n│   │   ├── validation/\n│   │   │       ├── benign/\n│   │   │       └── malignant/\n│   │   └── test/\n│   │           ├── benign/\n│   │           └── malignant/\n│   ├──masks/\n│   │   |── train/\n│   │   │       ├── benign/\n│   │   │       └── malignant/\n│   │   ├── validation/\n│   │   │       ├── benign/\n│   │   │       └── malignant/\n│   │   └── test/\n│   │           ├── benign/\n│   │           └── malignant/\n│\n└── kau_bcmd/\n│    ├── train/        (Paired mammo and US images, potentially linked by ID)\n│    ├── validation/   (Paired mammo and US images)\n│    └── test/         (Paired mammo and US images)\n```\n\nWithin each split (`train`, `validation`, `test`), images are further categorized by diagnosis (`benign`, `malignant`). The KAU-BCMD dataset, being used for multimodal evaluation, will contain paired images. The structure for KAU-BCMD might need to include a mapping or consistent naming convention to link the mammogram and ultrasound images for the same patient/case.\n\n## Dataset Contents\n\n* **Images:** DICOM or common image formats (PNG, JPG) for mammography and ultrasound images.\n* **Annotations:** Original annotations provided with the datasets, which may include bounding boxes, lesion types, and BI-RADS assessments.\n* **Labels:** Binary labels indicating the diagnosis (benign or malignant).\n\n## Usage\n\nThis dataset is intended for training and evaluating deep learning models for breast cancer diagnosis using multimodal imaging. The structured format facilitates easy loading and processing using standard deep learning libraries and Hugging Face's `datasets` library.\n\n## License\n\nPlease refer to the original licenses of the VinDr-Mammo, BUSI, and KAU-BCMD datasets for specific terms of use.\n\n## Citation\n\nPlease cite the original dataset sources when using this compiled dataset in your research.\n\n```bibtex\n@article{pham2022vindr,\n  title={VinDr-Mammo: A large-scale benchmark dataset for computer-aided detection and diagnosis in full-field digital mammography},\n  author={Pham, Hai H and Nguyen-Trung, Hieu and Nguyen, Hoang Q},\n  journal={PhysioNet},\n  year={2022}\n}\n\n@article{al2020dataset,\n  title={Dataset of breast ultrasound images},\n  author={Al-Dhabyani, Waleed and Gomaa, Mohamed and Khaled, Hossam and Fahmy, Amr},\n  journal={Data in Brief},\n  volume={28},\n  pages={104863},\n  year={2020},\n  publisher={Elsevier}\n}\n\n@article{alsolami2021king,\n  title={King Abdulaziz University Breast Cancer Mammogram Dataset (KAU-BCMD)},\n  author={Alsolami, Abdulrahman S and Shalash, Walid and Alsaggaf, Walid and Ashoor, Sara and Refaat, Hesham and Elmogy, Mohammed},\n  journal={Data},\n  volume={6},\n  number={11},\n  pages={111},\n  year={2021},\n  publisher={MDPI}\n}\n```","metadata":{"id":"mAxj0gW5zcGs"}},{"cell_type":"code","source":"# # prompt: code to add the markdown content to the README.md file. write the code in full, i would add the markdown content in a different section\n\n# from google.colab import userdata\n# import os\n\n\n# def add_markdown_to_readme(readme_path, markdown_content):\n#     \"\"\"Adds markdown content to the README.md file.\"\"\"\n#     try:\n#         with open(readme_path, 'a') as f:  # Open in append mode\n#             f.write(\"\\n\")  # Add a newline for separation\n#             f.write(markdown_content)\n#         print(f\"Markdown content successfully added to {readme_path}\")\n#     except FileNotFoundError:\n#         print(f\"README.md file not found at {readme_path}\")\n#     except Exception as e:\n#         print(f\"An error occurred: {e}\")\n\n\n# if __name__ == \"__main__\":\n#     # Markdown content to add (replace with your actual content)\n#     markdown_content = \"\"\"\n# # Multimodal Breast Cancer Imaging Dataset\n\n# This dataset combines publicly available mammography and ultrasound imaging data for the development and evaluation of multimodal deep learning models for breast cancer diagnosis, with a focus on interpretability and applicability in resource-limited settings.\n\n# ## Dataset Sources\n\n# This dataset is compiled from the following publicly available sources:\n\n# * **VinDr-Mammo:** A large-scale benchmark dataset for computer-aided detection and diagnosis in full-field digital mammography.\n#     * **Source:** [https://doi.org/10.13026/br2v-7517](https://doi.org/10.13026/br2v-7517)\n#     * **Reference:** Pham, H. H., Nguyen-Trung, H., & Nguyen, H. Q. (2022). VinDr-Mammo: A large-scale benchmark dataset for computer-aided detection and diagnosis in full-field digital mammography (version 1.0.0). *PhysioNet*.\n\n# * **BUSI (Breast Ultrasound Images) Dataset:** A well-curated collection of breast ultrasound images.\n#     * **Source:** [https://doi.org/10.1016/j.dib.2019.104863](https://doi.org/10.1016/j.dib.2019.104863)\n#     * **Reference:** Al-Dhabyani, W., Gomaa, M., Khaled, H., & Fahmy, A. (2020). Dataset of breast ultrasound images. *Data in Brief*, 28, 104863.\n\n# * **KAU-BCMD (King Abdulaziz University Breast Cancer Mammogram Dataset):** Includes paired mammography and ultrasound images for a subset of cases, valuable for multimodal evaluation.\n#     * **Source:** [https://doi.org/10.3390/data6110111](https://doi.3390/data6110111)\n#     * **Reference:** Alsolami, A. S., Shalash, W., Alsaggaf, W., Ashoor, S., Refaat, H., & Elmogy, M. (2021). King Abdulaziz University Breast Cancer Mammogram Dataset (KAU-BCMD). *Data*, 6(11), 111.\n\n# ## Dataset Structure\n\n# The dataset is organized into folders based on modality and data split. The proposed structure is as follows:\n# ```\n# .mmibc/\n# ├── mammo/\n# │   ├── train/\n# │   │   ├── benign/\n# │   │   └── malignant/\n# │   ├── validation/\n# │   │   ├── benign/\n# │   │   └── malignant/\n# │   └── test/\n# │       ├── benign/\n# │       └── malignant/\n# ├── busi/\n# │   ├──images/\n# │   │   |── train/\n# │   │   │       ├── benign/\n# │   │   │       └── malignant/\n# │   │   ├── validation/\n# │   │   │       ├── benign/\n# │   │   │       └── malignant/\n# │   │   └── test/\n# │   │           ├── benign/\n# │   │           └── malignant/\n# │   ├──masks/\n# │   │   |── train/\n# │   │   │       ├── benign/\n# │   │   │       └── malignant/\n# │   │   ├── validation/\n# │   │   │       ├── benign/\n# │   │   │       └── malignant/\n# │   │   └── test/\n# │   │           ├── benign/\n# │   │           └── malignant/\n# │\n# └── kau_bcmd/\n# │    ├── train/        (Paired mammo and US images, potentially linked by ID)\n# │    ├── validation/   (Paired mammo and US images)\n# │    └── test/         (Paired mammo and US images)\n# ```\n\n# Within each split (`train`, `validation`, `test`), images are further categorized by diagnosis (`benign`, `malignant`). The KAU-BCMD dataset, being used for multimodal evaluation, will contain paired images. The structure for KAU-BCMD might need to include a mapping or consistent naming convention to link the mammogram and ultrasound images for the same patient/case.\n\n# ## Dataset Contents\n\n# * **Images:** DICOM or common image formats (PNG, JPG) for mammography and ultrasound images.\n# * **Annotations:** Original annotations provided with the datasets, which may include bounding boxes, lesion types, and BI-RADS assessments.\n# * **Labels:** Binary labels indicating the diagnosis (benign or malignant).\n\n# ## Usage\n\n# This dataset is intended for training and evaluating deep learning models for breast cancer diagnosis using multimodal imaging. The structured format facilitates easy loading and processing using standard deep learning libraries and Hugging Face's `datasets` library.\n\n# ## License\n\n# Please refer to the original licenses of the VinDr-Mammo, BUSI, and KAU-BCMD datasets for specific terms of use.\n\n# ## Citation\n\n# Please cite the original dataset sources when using this compiled dataset in your research.\n\n# ```bibtex\n# @article{pham2022vindr,\n#   title={VinDr-Mammo: A large-scale benchmark dataset for computer-aided detection and diagnosis in full-field digital mammography},\n#   author={Pham, Hai H and Nguyen-Trung, Hieu and Nguyen, Hoang Q},\n#   journal={PhysioNet},\n#   year={2022}\n# }\n\n# @article{al2020dataset,\n#   title={Dataset of breast ultrasound images},\n#   author={Al-Dhabyani, Waleed and Gomaa, Mohamed and Khaled, Hossam and Fahmy, Amr},\n#   journal={Data in Brief},\n#   volume={28},\n#   pages={104863},\n#   year={2020},\n#   publisher={Elsevier}\n# }\n\n# @article{alsolami2021king,\n#   title={King Abdulaziz University Breast Cancer Mammogram Dataset (KAU-BCMD)},\n#   author={Alsolami, Abdulrahman S and Shalash, Walid and Alsaggaf, Walid and Ashoor, Sara and Refaat, Hesham and Elmogy, Mohammed},\n#   journal={Data},\n#   volume={6},\n#   number={11},\n#   pages={111},\n#   year={2021},\n#   publisher={MDPI}\n# }\n# ```\n#     \"\"\"\n\n#     # Path to the README.md file in your cloned repository\n#     readme_path = \"/content/mmibc/README.md\"\n#     add_markdown_to_readme(readme_path, markdown_content)\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lz9Bj-iq1IhD","outputId":"5adeb09f-6b10-4673-fdaa-18c69faa857c","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# !python /content/MMIBC/src/data_handling/huggingface_upload.py","metadata":{"id":"lJdZLfts166x","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Data Preparation Procedure","metadata":{"id":"7m4IJY-BLax0"}},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport logging\nfrom pathlib import Path\n\n# Set up basic logging\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\nlogger = logging.getLogger(__name__)\n\n# --- Configuration - Update these paths and details ---\n# Base directory where your organized data is located\nbase_organized_dir = './mmibc' # Update this path\n\n# Directories for preprocessed images within the organized structure\n# Based on your provided structure: ./mmibc/busi/images/split/class/\nbusi_images_base_dir = os.path.join(base_organized_dir, 'busi', 'images')\n# Based on your provided structure: ./mmibc/vindr_mammo/split/class/\nvindr_mammo_base_dir = os.path.join(base_organized_dir, 'vindr_mammo')\n\n\n# Output path for the combined unimodal metadata CSV\ncombined_unimodal_metadata_output_path = os.path.join(base_organized_dir, 'combined_unimodal_metadata.csv')\n\n# Define the splits you have in your organized data\nsplits = ['train', 'validation', 'test']\n# Define the classes you have in your organized data\nclasses = ['benign', 'malignant'] # Assuming binary classification\n\n# --- Generate Metadata for Unimodal Datasets ---\nunimodal_metadata_list = []\n\nlogger.info(\"Generating unimodal metadata from organized BUSI and VinDr-Mammo directories...\")\n\n# Process BUSI images\nmodality = 'ultrasound' # Or 'BUSI'\nfor split in splits:\n    for class_name in classes:\n        class_dir = os.path.join(busi_images_base_dir, split, class_name)\n        if not os.path.exists(class_dir):\n            logger.warning(f\"Directory not found: {class_dir}. Skipping.\")\n            continue\n\n        for filename in os.listdir(class_dir):\n            if filename.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tiff')):\n                image_path = os.path.join(class_dir, filename)\n                label = 1 if class_name == 'malignant' else 0 # Assuming 'malignant' is class 1\n\n                unimodal_metadata_list.append({\n                    'image_path': image_path,\n                    'label': label,\n                    'split': split,\n                    'modality': modality,\n                    'class_name': class_name,\n                    'filename': filename\n                    # Add patient_id if you can extract it from the filename or structure\n                    # 'patient_id': extract_patient_id(filename) # You need to implement this\n                })\nlogger.info(f\"Collected {len(unimodal_metadata_list)} entries from BUSI.\")\n\n\n# Process VinDr-Mammo images\nmodality = 'mammography' # Or 'VinDr-Mammo'\nfor split in splits:\n    for class_name in classes:\n        class_dir = os.path.join(vindr_mammo_base_dir, split, class_name)\n        if not os.path.exists(class_dir):\n            logger.warning(f\"Directory not found: {class_dir}. Skipping.\")\n            continue\n\n        for filename in os.listdir(class_dir):\n            if filename.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tiff')):\n                image_path = os.path.join(class_dir, filename)\n                label = 1 if class_name == 'malignant' else 0 # Assuming 'malignant' is class 1\n\n                unimodal_metadata_list.append({\n                    'image_path': image_path,\n                    'label': label,\n                    'split': split,\n                    'modality': modality,\n                    'class_name': class_name,\n                    'filename': filename\n                    # Add patient_id if you can extract it from the filename or structure\n                    # 'patient_id': extract_patient_id(filename) # You need to implement this\n                })\nlogger.info(f\"Collected {len(unimodal_metadata_list) - len([e for e in unimodal_metadata_list if e['modality'] == 'ultrasound'])} entries from VinDr-Mammo.\")\n\n\n# Create DataFrame\nunimodal_metadata_df = pd.DataFrame(unimodal_metadata_list)\n\n# --- Save the Combined Unimodal Metadata CSV ---\nPath(os.path.dirname(combined_unimodal_metadata_output_path)).mkdir(parents=True, exist_ok=True) # Ensure output directory exists\nunimodal_metadata_df.to_csv(combined_unimodal_metadata_output_path, index=False)\n\nlogger.info(f\"Combined unimodal metadata CSV generated successfully at: {combined_unimodal_metadata_output_path}\")\nlogger.info(f\"Total unimodal samples found: {len(unimodal_metadata_df)}\")\n\n# --- Note on Multimodal Training ---\nlogger.warning(\"\\n--- IMPORTANT NOTE FOR MULTIMODAL TRAINING ---\")\nlogger.warning(\"This generated metadata lists UNIMODAL images (either mammography or ultrasound).\")\nlogger.warning(\"The MultiModalBreastCancerDataset requires PAIRED mammogram and ultrasound images for the same patient/case in each row.\")\nlogger.warning(\"To train a multimodal model, you need a dataset (like KAU-BCMD) that provides paired images, or you need to create pairs from unimodal datasets if possible (which is generally not feasible without specific pairing information).\")\nlogger.warning(\"You will need a different metadata structure and potentially a different Dataset class if you intend to train unimodal models or use this data for unimodal pre-training.\")\n\n","metadata":{"id":"j_c3INN02xDd","colab":{"base_uri":"https://localhost:8080/"},"outputId":"100d9566-5a8c-441d-c2cc-eec60428e574","trusted":true,"execution":{"iopub.status.busy":"2025-05-19T10:58:14.414437Z","iopub.execute_input":"2025-05-19T10:58:14.415288Z","iopub.status.idle":"2025-05-19T10:58:14.890815Z","shell.execute_reply.started":"2025-05-19T10:58:14.415247Z","shell.execute_reply":"2025-05-19T10:58:14.890201Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/working/mmibc/combined_unimodal_metadata.csv\")","metadata":{"id":"m8R0DeBNLaCb","trusted":true,"execution":{"iopub.status.busy":"2025-05-19T10:58:15.691126Z","iopub.execute_input":"2025-05-19T10:58:15.691722Z","iopub.status.idle":"2025-05-19T10:58:15.750392Z","shell.execute_reply.started":"2025-05-19T10:58:15.691690Z","shell.execute_reply":"2025-05-19T10:58:15.749851Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"df.head()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":313},"id":"lFjRu4ToqScp","outputId":"62ac7946-a43a-4be4-a4c1-e22c499c2855","trusted":true,"execution":{"iopub.status.busy":"2025-05-19T10:58:15.751324Z","iopub.execute_input":"2025-05-19T10:58:15.751490Z","iopub.status.idle":"2025-05-19T10:58:15.762560Z","shell.execute_reply.started":"2025-05-19T10:58:15.751476Z","shell.execute_reply":"2025-05-19T10:58:15.761846Z"}},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"                                          image_path  label  split  \\\n0  ./mmibc/busi/images/train/benign/benign (246).png      0  train   \n1  ./mmibc/busi/images/train/benign/benign (127).png      0  train   \n2  ./mmibc/busi/images/train/benign/benign (135).png      0  train   \n3  ./mmibc/busi/images/train/benign/benign (313).png      0  train   \n4  ./mmibc/busi/images/train/benign/benign (337).png      0  train   \n\n     modality class_name          filename  \n0  ultrasound     benign  benign (246).png  \n1  ultrasound     benign  benign (127).png  \n2  ultrasound     benign  benign (135).png  \n3  ultrasound     benign  benign (313).png  \n4  ultrasound     benign  benign (337).png  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image_path</th>\n      <th>label</th>\n      <th>split</th>\n      <th>modality</th>\n      <th>class_name</th>\n      <th>filename</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>./mmibc/busi/images/train/benign/benign (246).png</td>\n      <td>0</td>\n      <td>train</td>\n      <td>ultrasound</td>\n      <td>benign</td>\n      <td>benign (246).png</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>./mmibc/busi/images/train/benign/benign (127).png</td>\n      <td>0</td>\n      <td>train</td>\n      <td>ultrasound</td>\n      <td>benign</td>\n      <td>benign (127).png</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>./mmibc/busi/images/train/benign/benign (135).png</td>\n      <td>0</td>\n      <td>train</td>\n      <td>ultrasound</td>\n      <td>benign</td>\n      <td>benign (135).png</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>./mmibc/busi/images/train/benign/benign (313).png</td>\n      <td>0</td>\n      <td>train</td>\n      <td>ultrasound</td>\n      <td>benign</td>\n      <td>benign (313).png</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>./mmibc/busi/images/train/benign/benign (337).png</td>\n      <td>0</td>\n      <td>train</td>\n      <td>ultrasound</td>\n      <td>benign</td>\n      <td>benign (337).png</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"# Import necessary libraries\nimport os\nimport pandas as pd\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.transforms as transforms\nimport cv2\nimport logging\nimport random\nimport numpy as np # Needed for DataAugmentation and potential numpy operations\nfrom PIL import Image\n\n# Set up basic logging\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\nlogger = logging.getLogger(__name__)\n\n# --- Assume DataAugmentation Class is Available ---\n# Copy the DataAugmentation class from model/dino_nn.py into a cell above this,\n# or ensure model/dino_nn.py is in your Python path and import it.\n# from model.dino_nn import DataAugmentation\n\nclass DataAugmentation:\n    \"\"\"\n    Data augmentation for DINOv2-like training.\n    Creates multiple crops of different sizes.\n    Can be adapted for modality-specific transforms if needed.\n    \"\"\"\n    def __init__(self, global_crops_scale=(0.5, 1.0), local_crops_scale=(0.2, 0.5),\n                 local_crops_number=8, global_size=224, local_size=96, in_chans=1):\n        self.global_crops_scale = global_crops_scale\n        self.local_crops_scale = local_crops_scale\n        self.local_crops_number = local_crops_number\n        self.global_size = global_size\n        self.local_size = local_size\n        self.in_chans = in_chans\n\n        if self.in_chans == 1:\n             mean = [0.485]\n             std = [0.229]\n        else:\n             mean = [0.485, 0.456, 0.406]\n             std = [0.229, 0.224, 0.225]\n\n        # Define base transforms that are common across modalities\n        # Modality-specific adjustments would happen within the __call__ method\n        self.base_global_transform = transforms.Compose([\n            transforms.RandomResizedCrop(global_size, scale=global_crops_scale,\n                                         interpolation=transforms.InterpolationMode.BICUBIC),\n            transforms.RandomHorizontalFlip(p=0.5),\n            # transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.2, hue=0.1), # Optional, potentially less suitable for medical\n        ])\n\n        self.base_local_transform = transforms.Compose([\n            transforms.RandomResizedCrop(local_size, scale=local_crops_scale,\n                                         interpolation=transforms.InterpolationMode.BICUBIC),\n            transforms.RandomHorizontalFlip(p=0.5),\n             # transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.2, hue=0.1), # Optional, potentially less suitable for medical\n        ])\n\n        self.to_tensor_normalize = transforms.Compose([\n             transforms.ToTensor(),\n             transforms.Normalize(mean=mean, std=std)\n        ])\n\n\n    def __call__(self, image: Image.Image, modality: str):\n        \"\"\"\n        Apply different transformations to create multiple views (crops) of the same image,\n        potentially with modality-specific adjustments.\n\n        Args:\n            image (PIL.Image): The input image (expected as PIL Image).\n            modality (str): The modality of the image ('mammography' or 'ultrasound').\n\n        Returns:\n            list: A list of transformed image tensors (crops).\n        \"\"\"\n        crops = []\n\n        # --- Apply Modality-Specific Adjustments (Example) ---\n        # You can add modality-specific augmentations here before the base transforms\n        # if needed. For example:\n        if modality == 'mammography':\n            # Apply specific augmentations for mammography\n            image = transforms.functional.adjust_contrast(image, contrast_factor=random.uniform(0.8, 1.2))\n        elif modality == 'ultrasound':\n            # Apply specific augmentations for ultrasound\n            image = transforms.functional.adjust_brightness(image, brightness_factor=random.uniform(0.8, 1.2))\n        # --- End Modality-Specific Adjustments ---\n\n\n        # Apply global transforms (2 crops)\n        for _ in range(2):\n            crop = self.base_global_transform(image)\n            crops.append(self.to_tensor_normalize(crop)) # Apply ToTensor and Normalize after augmentation\n\n        # Apply local transforms (local_crops_number crops)\n        for _ in range(self.local_crops_number):\n            crop = self.base_local_transform(image)\n            crops.append(self.to_tensor_normalize(crop)) # Apply ToTensor and Normalize after augmentation\n\n        return crops\n\n\n","metadata":{"id":"xC5CnICZunTg","trusted":true,"execution":{"iopub.status.busy":"2025-05-19T10:58:15.868117Z","iopub.execute_input":"2025-05-19T10:58:15.868323Z","iopub.status.idle":"2025-05-19T10:58:19.057812Z","shell.execute_reply.started":"2025-05-19T10:58:15.868306Z","shell.execute_reply":"2025-05-19T10:58:19.057203Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"class UnimodalBCDataset(Dataset):\n    \"\"\"\n    Custom Dataset for loading unimodal medical images for pre-training.\n    Loads data based on a metadata CSV file and applies multi-crop augmentation.\n    Includes modality information.\n    \"\"\"\n    def __init__(self, metadata_path: str, transform: DataAugmentation):\n        \"\"\"\n        Args:\n            metadata_path: Path to the CSV file containing unimodal image metadata.\n                           Expected columns: 'image_path', 'modality'\n            transform: The DataAugmentation transform for multi-cropping.\n        \"\"\"\n        logger.info(f\"Loading unimodal dataset from metadata: {metadata_path}\")\n        if not os.path.exists(metadata_path):\n            logger.error(f\"Metadata file not found at {metadata_path}\")\n            raise FileNotFoundError(f\"Metadata file not found at {metadata_path}\")\n\n        self.metadata_df = pd.read_csv(metadata_path)\n        self.transform = transform\n\n        # Validate required columns\n        required_cols = ['image_path', 'modality']\n        if not all(col in self.metadata_df.columns for col in required_cols):\n            logger.error(f\"Metadata CSV must contain columns: {required_cols}\")\n            raise ValueError(f\"Metadata CSV must contain columns: {required_cols}\")\n\n        # Filter out rows with missing file paths or modality info\n        initial_samples = len(self.metadata_df)\n        self.metadata_df.dropna(subset=required_cols, inplace=True)\n        if len(self.metadata_df) < initial_samples:\n            logger.warning(f\"Removed {initial_samples - len(self.metadata_df)} samples due to missing data in metadata.\")\n\n\n        logger.info(f\"Unimodal dataset loaded with {len(self.metadata_df)} samples.\")\n\n    def __len__(self):\n        return len(self.metadata_df)\n\n    def __getitem__(self, idx):\n        if torch.is_tensor(idx):\n            idx = idx.tolist()\n\n        sample_info = self.metadata_df.iloc[idx]\n        img_path = sample_info['image_path']\n        modality = sample_info['modality'] # Get modality information\n        # Labels are not used in DINO pre-training, but Dataset requires returning something\n        dummy_label = 0\n\n        try:\n            # Load image (assuming grayscale based on your model config)\n            # Use cv2.IMREAD_GRAYSCALE to ensure 1 channel\n            img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n\n            if img is None:\n                 logger.error(f\"Could not load image: {img_path}. Returning random sample.\")\n                 return self.__getitem__(random.randint(0, len(self) - 1)) # Return random sample on error\n\n            # Convert numpy array to PIL Image for torchvision transforms\n            img_pil = Image.fromarray(img, mode='L') # 'L' for grayscale\n\n\n        except Exception as e:\n            logger.error(f\"Error loading image for index {idx} ({img_path}): {e}. Returning random sample.\")\n            return self.__getitem__(random.randint(0, len(self) - 1))\n\n\n        # Apply the DataAugmentation transform (multi-cropping)\n        # Pass the modality information to the transform\n        crops = self.transform(img_pil, modality)\n\n        # Return the list of crops and a dummy label\n        return crops, torch.tensor(dummy_label)\n","metadata":{"id":"sLxVUvKCqTy3","trusted":true,"execution":{"iopub.status.busy":"2025-05-19T10:58:19.059111Z","iopub.execute_input":"2025-05-19T10:58:19.059533Z","iopub.status.idle":"2025-05-19T10:58:19.068164Z","shell.execute_reply.started":"2025-05-19T10:58:19.059504Z","shell.execute_reply":"2025-05-19T10:58:19.067576Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"unimodal_metadata_csv_path = './mmibc/combined_unimodal_metadata.csv' # Update this path\n\n# Define parameters for Data Augmentation (should match your model/pre-training config)\nglobal_size = 224 # Base size of global crops (should match ViT input size)\nlocal_size = 96   # Size of local crops\nlocal_crops_number = 8 # Number of local crops\nin_chans = 1 # Grayscale\n","metadata":{"id":"UrJZj8phu5TK","trusted":true,"execution":{"iopub.status.busy":"2025-05-19T10:58:19.068998Z","iopub.execute_input":"2025-05-19T10:58:19.069251Z","iopub.status.idle":"2025-05-19T10:58:19.113225Z","shell.execute_reply.started":"2025-05-19T10:58:19.069226Z","shell.execute_reply":"2025-05-19T10:58:19.112692Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# Create the DataAugmentation transform instance\ndino_transform = DataAugmentation(\n    global_size=global_size,\n    local_size=local_size,\n    local_crops_number=local_crops_number,\n    in_chans=in_chans\n)","metadata":{"id":"HqjVA87gu3fe","trusted":true,"execution":{"iopub.status.busy":"2025-05-19T10:58:19.114982Z","iopub.execute_input":"2025-05-19T10:58:19.115189Z","iopub.status.idle":"2025-05-19T10:58:19.129674Z","shell.execute_reply.started":"2025-05-19T10:58:19.115173Z","shell.execute_reply":"2025-05-19T10:58:19.128939Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# Create the unimodal dataset instance\nunimodal_dataset = UnimodalBCDataset(\n    metadata_path=unimodal_metadata_csv_path,\n    transform=dino_transform\n)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"i_hJ5SFHvA6v","outputId":"25cd3284-7446-4452-9ad5-462d581c9c44","trusted":true,"execution":{"iopub.status.busy":"2025-05-19T10:58:19.130324Z","iopub.execute_input":"2025-05-19T10:58:19.130577Z","iopub.status.idle":"2025-05-19T10:58:19.202464Z","shell.execute_reply.started":"2025-05-19T10:58:19.130557Z","shell.execute_reply":"2025-05-19T10:58:19.201996Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# Define DataLoader parameters\nbatch_size = 64 # Batch size for pre-training (can be larger)\nnum_workers = 8 # Adjust based on your system's capabilities\n\n# Create the DataLoader instance\nunimodal_dataloader = DataLoader(\n    unimodal_dataset,\n    batch_size=batch_size,\n    shuffle=True, # Shuffle data for training\n    num_workers=num_workers,\n    pin_memory=True, # Pin memory for faster GPU transfer\n    drop_last=True # Drop the last incomplete batch (common in pre-training)\n)\n\n# --- You can now iterate through this dataloader for unimodal pre-training ---\nprint(f\"Number of unimodal samples in dataset: {len(unimodal_dataset)}\")\nprint(f\"Number of batches per epoch for pre-training: {len(unimodal_dataloader)}\")\n\n# Example of getting one batch\ntry:\n    crops_batch, dummy_labels = next(iter(unimodal_dataloader))\n    print(f\"\\nExample batch:\")\n    print(f\"Number of crops per image: {len(crops_batch)}\")\n    print(f\"Shape of first global crop batch: {crops_batch[0].shape}\")\n    print(f\"Shape of second global crop batch: {crops_batch[1].shape}\")\n    print(f\"Shape of first local crop batch: {crops_batch[2].shape}\")\n    print(f\"Dummy labels shape: {dummy_labels.shape}\")\n\nexcept StopIteration:\n    print(\"\\nDataLoader is empty.\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Hr4YXJykvE14","outputId":"2d3241af-8f91-45bf-b03f-1525f61f035c","trusted":true,"execution":{"iopub.status.busy":"2025-05-19T10:58:19.203116Z","iopub.execute_input":"2025-05-19T10:58:19.203318Z","iopub.status.idle":"2025-05-19T10:58:38.015551Z","shell.execute_reply.started":"2025-05-19T10:58:19.203302Z","shell.execute_reply":"2025-05-19T10:58:38.014478Z"}},"outputs":[{"name":"stdout","text":"Number of unimodal samples in dataset: 20906\nNumber of batches per epoch for pre-training: 326\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\nExample batch:\nNumber of crops per image: 10\nShape of first global crop batch: torch.Size([64, 1, 224, 224])\nShape of second global crop batch: torch.Size([64, 1, 224, 224])\nShape of first local crop batch: torch.Size([64, 1, 96, 96])\nDummy labels shape: torch.Size([64])\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# prompt: show tensorboard within colab.\n\n%load_ext tensorboard\n%tensorboard --logdir /content/dino_unimodal_pretraining_output\n","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":856},"id":"yjnZ0ZdH0PZu","outputId":"d299645e-bc2f-4960-8191-0285428cb43f","trusted":true,"execution":{"iopub.status.busy":"2025-05-19T10:58:38.016808Z","iopub.execute_input":"2025-05-19T10:58:38.017149Z","iopub.status.idle":"2025-05-19T10:58:44.058530Z","shell.execute_reply.started":"2025-05-19T10:58:38.017112Z","shell.execute_reply":"2025-05-19T10:58:44.057533Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Javascript object>","application/javascript":"\n        (async () => {\n            const url = new URL(await google.colab.kernel.proxyPort(6006, {'cache': true}));\n            url.searchParams.set('tensorboardColab', 'true');\n            const iframe = document.createElement('iframe');\n            iframe.src = url;\n            iframe.setAttribute('width', '100%');\n            iframe.setAttribute('height', '800');\n            iframe.setAttribute('frameborder', 0);\n            document.body.appendChild(iframe);\n        })();\n    "},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"# --- Necessary Imports ---\nimport copy\nimport math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision.transforms as transforms\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch import optim\nimport torch.distributed as dist\nimport numpy as np\nimport argparse\nimport os\nimport shutil # For save_checkpoint\nimport time # For timing\nfrom torch.utils.tensorboard import SummaryWriter # For TensorBoard logging\nimport datetime # For MetricLogger eta\nimport pandas as pd # For metadata handling\nimport cv2 # For image loading\nimport logging # For logging\nimport random # For random sample loading on error\nfrom PIL import Image # Import PIL for image handling\nfrom pathlib import Path # For directory creation\n\n# Set up logging (adjust level as needed)\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(levelname)s - %(message)s'\n)\nlogger = logging.getLogger(__name__)\n\n# --- Utility Functions (Copied from utils.py) ---\n\nclass MetricLogger:\n    \"\"\"\n    Utility class for logging metrics during training\n    \"\"\"\n    def __init__(self, delimiter=\"\\t\"):\n        self.meters = {}\n        self.delimiter = delimiter\n\n    def update(self, **kwargs):\n        for k, v in kwargs.items():\n            if k not in self.meters:\n                self.meters[k] = SmoothedValue()\n            if isinstance(v, torch.Tensor):\n                v = v.item()\n            self.meters[k].update(v)\n\n    def __getattr__(self, attr):\n        if attr in self.meters:\n            return self.meters[attr]\n        # Default behavior\n        raise AttributeError(f\"'{type(self).__name__}' object has no attribute '{attr}'\")\n\n    def __str__(self):\n        loss_str = []\n        for name, meter in self.meters.items():\n            loss_str.append(f\"{name}: {meter}\")\n        return self.delimiter.join(loss_str)\n\n    def synchronize_between_processes(self):\n        # For multi-GPU training - Placeholder\n        pass\n\n    def log_every(self, iterable, print_freq, header=None):\n        i = 0\n        if header is not None:\n            print(header)\n\n        start_time = time.time()\n        end = time.time()\n        for obj in iterable:\n            data_time = time.time() - end\n            yield obj\n            batch_time = time.time() - end\n            end = time.time()\n            if i % print_freq == 0:\n                eta_seconds = (len(iterable) - i) * (batch_time + data_time) / 2\n                eta_string = str(datetime.timedelta(seconds=int(eta_seconds)))\n                print(\n                    f\"{header} [{i}/{len(iterable)}]\\t\"\n                    f\"eta: {eta_string}\\t\"\n                    f\"time: {batch_time:.4f}\\t\"\n                    f\"data: {data_time:.4f}\\t\"\n                    f\"{self}\"\n                )\n            i += 1\n        total_time = time.time() - start_time\n        total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n        print(f'{header} Total time: {total_time_str} ({total_time / len(iterable):.4f}s / it)')\n","metadata":{"id":"_htx9Nmi2s4o","trusted":true,"execution":{"iopub.status.busy":"2025-05-19T10:58:44.059457Z","iopub.execute_input":"2025-05-19T10:58:44.059723Z","iopub.status.idle":"2025-05-19T10:58:48.002027Z","shell.execute_reply.started":"2025-05-19T10:58:44.059703Z","shell.execute_reply":"2025-05-19T10:58:48.001441Z"}},"outputs":[{"name":"stderr","text":"2025-05-19 10:58:44.797079: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1747652324.819559     531 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1747652324.826486     531 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"class SmoothedValue:\n    \"\"\"\n    Track a series of values and provide access to smoothed values over a window\n    \"\"\"\n    def __init__(self, window_size=20):\n        self.window_size = window_size\n        self.reset()\n\n    def reset(self):\n        self.values = []\n        self.total = 0.0\n        self.count = 0\n\n    def update(self, value):\n        self.values.append(value)\n        self.total += value\n        self.count += 1\n        if len(self.values) > self.window_size:\n            self.total -= self.values.pop(0)\n\n    @property\n    def median(self):\n        return np.median(self.values)\n\n    @property\n    def avg(self):\n        return np.mean(self.values)\n\n    @property\n    def global_avg(self):\n        return self.total / self.count\n\n    def __str__(self):\n        return f\"{self.global_avg:.4f} ({self.avg:.4f})\"\n\ndef cosine_scheduler(base_value, final_value, epochs, niter_per_ep, warmup_epochs=0, start_warmup_value=0):\n    \"\"\"\n    Cosine scheduler with warmup for updating parameters like teacher momentum\n    \"\"\"\n    warmup_schedule = np.linspace(start_warmup_value, base_value, warmup_epochs * niter_per_ep)\n\n    iters = np.arange(epochs * niter_per_ep - warmup_epochs * niter_per_ep)\n    schedule = final_value + 0.5 * (base_value - final_value) * (1 + np.cos(np.pi * iters / len(iters)))\n\n    schedule = np.concatenate((warmup_schedule, schedule))\n    assert len(schedule) == epochs * niter_per_ep\n    return schedule\n\ndef save_checkpoint(state, is_best, filename='checkpoint.pth', save_dir='.'):\n    \"\"\"\n    Saves model checkpoint.\n    \"\"\"\n    filepath = os.path.join(save_dir, filename)\n    torch.save(state, filepath)\n    logger.info(f\"Checkpoint saved to {filepath}\")\n    if is_best:\n        best_filepath = os.path.join(save_dir, 'model_best.pth')\n        shutil.copyfile(filepath, best_filepath)\n        logger.info(f\"Best model saved to {best_filepath}\")","metadata":{"id":"ERBsEcJm2zMm","trusted":true,"execution":{"iopub.status.busy":"2025-05-19T10:58:48.002768Z","iopub.execute_input":"2025-05-19T10:58:48.003186Z","iopub.status.idle":"2025-05-19T10:58:48.011915Z","shell.execute_reply.started":"2025-05-19T10:58:48.003166Z","shell.execute_reply":"2025-05-19T10:58:48.011157Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"class DropPath(nn.Module):\n    \"\"\"\n    Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).\n    From https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/layers/drop.py\n    \"\"\"\n    def __init__(self, drop_prob: float = 0.):\n        super(DropPath, self).__init__()\n        self.drop_prob = drop_prob\n\n    def forward(self, x):\n        if self.drop_prob == 0. or not self.training:\n            return x\n        keep_prob = 1 - self.drop_prob\n        # work with diff dim tensors, not just 2D ConvNets\n        shape = (x.shape[0],) + (1,) * (x.ndim - 1)\n        random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n        random_tensor.floor_()  # binarize\n        output = x.div(keep_prob) * random_tensor\n        return output\n\n\nclass PatchEmbedding(nn.Module):\n    \"\"\"\n    2D Image to Patch Embedding with positional encoding\n    Handles variable input image sizes by interpolating positional embeddings.\n    \"\"\"\n    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):\n        super().__init__()\n        self.img_size = img_size\n        self.patch_size = patch_size\n        # Calculate the number of patches for the *expected* input size\n        self.num_patches = (img_size // patch_size) ** 2\n\n        # Linear projection using Conv2d\n        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n\n        # Learnable CLS token\n        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n        # Learnable positional embedding for the *expected* number of patches + CLS token\n        self.pos_embed = nn.Parameter(torch.zeros(1, self.num_patches + 1, embed_dim))\n\n        # Initialize parameters\n        nn.init.trunc_normal_(self.cls_token, std=0.02)\n        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x (torch.Tensor): Input image tensor (B, C, H, W)\n\n        Returns:\n            torch.Tensor: Patch embeddings with CLS token and positional encoding (B, actual_num_patches + 1, embed_dim)\n        \"\"\"\n        B, C, H, W = x.shape\n\n        # Project patches\n        x = self.proj(x)  # (B, embed_dim, H_out, W_out)\n        _, _, H_out, W_out = x.shape\n        actual_num_patches = H_out * W_out\n\n        # Flatten spatial dimensions and transpose\n        x = x.flatten(2).transpose(1, 2) # (B, actual_num_patches, embed_dim)\n\n        # Interpolate positional embedding if the number of patches doesn't match the expected size\n        # This handles cases where input H or W are not perfectly divisible by patch_size,\n        # or when using crops of different sizes (global vs local).\n        if actual_num_patches != self.num_patches:\n            # Positional embedding excluding the CLS token\n            pos_embed_patches = self.pos_embed[:, 1:] # (1, self.num_patches, embed_dim)\n            # Reshape to spatial dimensions for interpolation\n            pos_embed_spatial = pos_embed_patches.reshape(1, int(math.sqrt(self.num_patches)), int(math.sqrt(self.num_patches)), -1).permute(0, 3, 1, 2) # (1, embed_dim, sqrt(num_patches), sqrt(num_patches))\n            # Interpolate to match the actual spatial dimensions after convolution\n            pos_embed_interpolated = F.interpolate(pos_embed_spatial, size=(H_out, W_out), mode='bicubic', align_corners=False)\n            # Reshape back to sequence format\n            pos_embed_interpolated = pos_embed_interpolated.flatten(2).transpose(1, 2) # (1, actual_num_patches, embed_dim)\n\n            # Add the interpolated positional embedding to the patches\n            x = x + pos_embed_interpolated\n\n            # Add CLS token and its corresponding positional embedding\n            cls_token = self.cls_token.expand(B, -1, -1)\n            # The positional embedding for the CLS token is the first element of self.pos_embed\n            pos_embed_cls = self.pos_embed[:, :1] # (1, 1, embed_dim)\n            cls_token = cls_token + pos_embed_cls # Add positional embedding to CLS token\n            x = torch.cat((cls_token, x), dim=1) # Concatenate CLS token with patches\n\n        else:\n            # If the number of patches matches the expected size, use the original positional embedding\n            # Add CLS token\n            cls_token = self.cls_token.expand(B, -1, -1)\n            # Concatenate CLS token with patch embeddings\n            x = torch.cat((cls_token, x), dim=1) # (B, num_patches + 1, embed_dim)\n\n            # Add positional embedding (including the CLS token's position)\n            x = x + self.pos_embed\n\n\n        return x\n\n\nclass MultiHeadAttention(nn.Module):\n    \"\"\"\n    Multi-head Self-Attention module\n    \"\"\"\n    def __init__(self, dim, num_heads=8, qkv_bias=True, attn_drop=0., proj_drop=0.):\n        super().__init__()\n        assert dim % num_heads == 0, 'dim should be divisible by num_heads'\n        self.num_heads = num_heads\n        head_dim = dim // num_heads\n        self.scale = head_dim ** -0.5 # Scaling factor for attention scores\n\n        # Linear projection for Query, Key, Value\n        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n        self.attn_drop = nn.Dropout(attn_drop) # Dropout for attention scores\n        self.proj = nn.Linear(dim, dim) # Output projection\n        self.proj_drop = nn.Dropout(proj_drop) # Dropout for output\n\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x (torch.Tensor): Input tensor (B, N, C) where N is sequence length, C is dimension\n\n        Returns:\n            tuple: Output tensor (B, N, C) and attention weights (B, num_heads, N, N)\n        \"\"\"\n        B, N, C = x.shape\n\n        # Project and reshape Q, K, V\n        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n        # Separate Q, K, V\n        q, k, v = qkv.unbind(0)  # make torchscript happy (cannot use tensor as tuple)\n\n        # Compute attention scores\n        # (B, num_heads, N, head_dim) @ (B, num_heads, head_dim, N) -> (B, num_heads, N, N)\n        attn = (q @ k.transpose(-2, -1)) * self.scale\n        # Apply softmax to get attention probabilities\n        attn = attn.softmax(dim=-1)\n        # Apply attention dropout\n        attn = self.attn_drop(attn)\n\n        # Apply attention to values\n        # (B, num_heads, N, N) @ (B, num_heads, N, head_dim) -> (B, num_heads, N, head_dim)\n        x = (attn @ v).transpose(1, 2) # Transpose heads and sequence length\n        # Reshape back to original dimension\n        x = x.reshape(B, N, C)\n\n        # Apply output projection and dropout\n        x = self.proj(x)\n        x = self.proj_drop(x)\n\n        return x, attn\n\nclass MLP(nn.Module):\n    \"\"\"\n    MLP as used in Vision Transformer, MLP-Mixer, etc.\n    \"\"\"\n    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n\n        # Two linear layers with activation and dropout\n        self.fc1 = nn.Linear(in_features, hidden_features)\n        self.act = act_layer()\n        self.fc2 = nn.Linear(hidden_features, out_features)\n        self.drop = nn.Dropout(drop)\n\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x (torch.Tensor): Input tensor (B, N, in_features)\n\n        Returns:\n            torch.Tensor: Output tensor (B, N, out_features)\n        \"\"\"\n        x = self.fc1(x)\n        x = self.act(x)\n        x = self.drop(x)\n        x = self.fc2(x)\n        x = self.drop(x)\n        return x\n\nclass LayerScale(nn.Module):\n    \"\"\"\n    Layer scale from CaiT and for DINOv2\n    \"\"\"\n    def __init__(self, dim, init_values=1e-5):\n        super().__init__()\n        # Learnable parameter gamma initialized to small values\n        self.gamma = nn.Parameter(init_values * torch.ones(dim))\n\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x (torch.Tensor): Input tensor (B, N, dim)\n\n        Returns:\n            torch.Tensor: Scaled output tensor\n        \"\"\"\n        return self.gamma * x\n\nclass ViTBlock(nn.Module):\n    \"\"\"\n    Vision Transformer Block with LayerScale and DropPath\n    \"\"\"\n    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=True,\n                 drop=0., attn_drop=0., drop_path=0., act_layer=nn.GELU,\n                 norm_layer=nn.LayerNorm, layer_scale_init_value=1e-5):\n        super().__init__()\n        # First normalization layer\n        self.norm1 = norm_layer(dim)\n        # Multi-Head Self-Attention module\n        self.attn = MultiHeadAttention(dim, num_heads=num_heads, qkv_bias=qkv_bias,\n                                    attn_drop=attn_drop, proj_drop=drop)\n        # LayerScale after attention (optional)\n        self.ls1 = LayerScale(dim, init_values=layer_scale_init_value) if layer_scale_init_value > 0 else nn.Identity()\n\n        # Second normalization layer\n        self.norm2 = norm_layer(dim)\n        # MLP (Feed-Forward) module\n        mlp_hidden_dim = int(dim * mlp_ratio)\n        self.mlp = MLP(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n        self.ls2 = LayerScale(dim, init_values=layer_scale_init_value) if layer_scale_init_value > 0 else nn.Identity()\n\n        # Stochastic depth (DropPath)\n        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x (torch.Tensor): Input tensor (B, N, dim)\n\n        Returns:\n            tuple: Output tensor (B, N, dim) and attention weights (B, num_heads, N, N) from the attention block\n        \"\"\"\n        # Attention block with residual connection, normalization, LayerScale, and DropPath\n        res = x\n        x_norm = self.norm1(x)\n        x_attn, attn = self.attn(x_norm)\n        x_attn = self.ls1(x_attn)\n        x = res + self.drop_path(x_attn)\n\n        # MLP block with residual connection, normalization, LayerScale, and DropPath\n        res = x\n        x_norm = self.norm2(x)\n        x_mlp = self.mlp(x_norm)\n        x_mlp = self.ls2(x_mlp)\n        x = res + self.drop_path(x_mlp)\n\n        return x, attn\n\nclass ViTEncoder(nn.Module):\n    \"\"\"\n    Vision Transformer Encoder (Backbone)\n    \"\"\"\n    def __init__(self, img_size=224, patch_size=16, in_chans=1, embed_dim=768, depth=12,\n                 num_heads=12, mlp_ratio=4., qkv_bias=True, drop_rate=0., attn_drop_rate=0.,\n                 drop_path_rate=0.1, norm_layer=nn.LayerNorm, layer_scale_init_value=1e-5):\n        super().__init__()\n\n        # Patch embedding layer\n        self.patch_embed = PatchEmbedding(\n            img_size=img_size,\n            patch_size=patch_size,\n            in_chans=in_chans,\n            embed_dim=embed_dim\n        )\n        num_patches = self.patch_embed.num_patches\n\n        # Transformer blocks (encoder layers)\n        # Stochastic depth decay rule\n        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]\n        self.blocks = nn.ModuleList([\n            ViTBlock(\n                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias,\n                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i],\n                norm_layer=norm_layer, layer_scale_init_value=layer_scale_init_value\n            )\n            for i in range(depth)\n        ])\n\n        # Final normalization layer\n        self.norm = norm_layer(embed_dim)\n\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x (torch.Tensor): Input image tensor (B, C, H, W)\n\n        Returns:\n            tuple: CLS token representation (B, embed_dim), all tokens (B, num_patches + 1, embed_dim),\n                   list of attention maps from each block\n        \"\"\"\n        # Apply patch embedding\n        x = self.patch_embed(x)\n\n        # Store attention maps from each block\n        attn_maps = []\n\n        # Apply transformer blocks sequentially\n        for block in self.blocks:\n            x, attn = block(x)\n            attn_maps.append(attn)\n\n        # Apply final normalization\n        x = self.norm(x)\n\n        # Extract the CLS token (the first token in the sequence)\n        cls_token = x[:, 0]\n\n        # Return CLS token, all tokens, and attention maps\n        return cls_token, x, attn_maps\n","metadata":{"id":"m93d8J8F2zDd","trusted":true,"execution":{"iopub.status.busy":"2025-05-19T10:58:48.014224Z","iopub.execute_input":"2025-05-19T10:58:48.014836Z","iopub.status.idle":"2025-05-19T10:58:48.041766Z","shell.execute_reply.started":"2025-05-19T10:58:48.014818Z","shell.execute_reply":"2025-05-19T10:58:48.041230Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"class DINOLoss(nn.Module):\n    \"\"\"\n    DINO loss from the paper \"Emerging Properties in Self-Supervised Vision Transformers\"\n    with improvements from DINOv2\n    \"\"\"\n    def __init__(self, out_dim, teacher_temp=0.04, student_temp=0.1, center_momentum=0.9):\n        super().__init__()\n        self.student_temp = student_temp\n        self.teacher_temp = teacher_temp\n        self.center_momentum = center_momentum\n        # Register buffer for the center, initialized to zeros\n        self.register_buffer(\"center\", torch.zeros(1, out_dim))\n\n    def forward(self, student_output, teacher_output, current_teacher_temp):\n        \"\"\"\n        Cross-entropy between softmax outputs of the teacher and student networks.\n\n        Args:\n            student_output (list): List of tensors, student outputs for each crop.\n            teacher_output (list): List of tensors, teacher outputs for the global crops.\n            current_teacher_temp (float): The current temperature for the teacher.\n\n        Returns:\n            torch.Tensor: The computed DINO loss.\n        \"\"\"\n        # Student outputs for all crops\n        student_out = [s / self.student_temp for s in student_output]\n\n        # Teacher outputs for global crops, with current temperature and sharpening\n        teacher_out = [t / current_teacher_temp for t in teacher_output]\n        teacher_out = [F.softmax(t, dim=-1).detach() for t in teacher_out]\n\n        # Center the teacher outputs\n        teacher_out = [t - self.center for t in teacher_out]\n\n        # Compute loss between global crops teacher and all crops student\n        total_loss = 0\n        n_crops = len(student_output)\n        n_global_crops = len(teacher_output) # Should be 2 for DINO\n\n        for s_idx in range(n_crops):\n            for t_idx in range(n_global_crops):\n                # Loss is cross-entropy between student crop s_idx and teacher crop t_idx\n                loss = -torch.sum(teacher_out[t_idx] * F.log_softmax(student_out[s_idx], dim=-1), dim=-1).mean()\n                total_loss += loss\n\n        # Average loss over all pairs of student and teacher global crops\n        total_loss /= (n_crops * n_global_crops)\n\n        # Update center for teacher output (using only global crops)\n        with torch.no_grad():\n            self.update_center(torch.cat(teacher_output, dim=0)) # Update center using concatenated global crops\n\n        return total_loss\n\n    @torch.no_grad()\n    def update_center(self, teacher_output):\n        \"\"\"\n        Update center used for teacher output.\n        \"\"\"\n        # Calculate batch center\n        batch_center = torch.sum(teacher_output, dim=0, keepdim=True)\n\n        # All-reduce across processes for distributed training\n        if dist.is_initialized():\n             dist.all_reduce(batch_center)\n             # Divide by total number of samples across all processes\n             batch_center = batch_center / (len(teacher_output) * dist.get_world_size())\n        else:\n             # If not in distributed mode, just divide by batch size\n             batch_center = batch_center / len(teacher_output)\n\n\n        # Update center using momentum\n        self.center = self.center * self.center_momentum + batch_center * (1 - self.center_momentum)\n\n\nclass DINOHead(nn.Module):\n    \"\"\"\n    Projection head used for DINO/DINOv2\n    \"\"\"\n    def __init__(self, in_dim, out_dim, hidden_dim=2048, bottleneck_dim=256, norm_last_layer=True):\n        super().__init__()\n\n        # MLP projection layers\n        self.mlp = nn.Sequential(\n            nn.Linear(in_dim, hidden_dim),\n            nn.GELU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.GELU(),\n            nn.Linear(hidden_dim, bottleneck_dim),\n        )\n\n        # Last FC layer mapping to output dimension\n        self.last_layer = nn.Linear(bottleneck_dim, out_dim, bias=False)\n\n        # Option to normalize the last layer weights\n        self.norm_last_layer = norm_last_layer\n\n        # Apply weight initialization\n        self.apply(self._init_weights)\n\n        # Normalize last layer weights if requested\n        if norm_last_layer:\n            nn.init.constant_(self.last_layer.weight, 0) # Initialize to zero for normalization\n\n    def _init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            nn.init.trunc_normal_(m.weight, std=0.02)\n            if m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x (torch.Tensor): Input tensor (B, in_dim) - typically the CLS token representation.\n\n        Returns:\n            torch.Tensor: Output tensor (B, out_dim) after projection and normalization.\n        \"\"\"\n        # Pass through MLP\n        x = self.mlp(x)\n        # Normalize the output of the bottleneck layer\n        x = F.normalize(x, dim=-1, p=2)\n\n        # Apply last layer\n        if self.norm_last_layer:\n            # Normalize weights before applying linear transformation\n            w = self.last_layer.weight.clone()\n            w = F.normalize(w, dim=1, p=2)\n            x = F.linear(x, w)\n        else:\n            x = self.last_layer(x)\n\n        return x\n\nclass MultiCropWrapper(nn.Module):\n    \"\"\"\n    Wrapper for processing multiple crops through the backbone and head\n    \"\"\"\n    def __init__(self, backbone: nn.Module, head: nn.Module):\n        super().__init__()\n        # Backbone network (e.g., ViTEncoder)\n        self.backbone = backbone\n        # Projection head (e.g., DINOHead)\n        self.head = head\n\n    def forward(self, x: list[torch.Tensor]):\n        \"\"\"\n        Args:\n            x (list): A list of image tensors, where each tensor is a different crop.\n\n        Returns:\n            list: A list of tensors, where each tensor is the output of the head for a crop.\n        \"\"\"\n        # Process each crop through the backbone and head\n        outputs = []\n        for crop in x:\n            # Get the CLS token from the backbone output\n            # Assuming backbone returns (cls_token, all_tokens, attention_maps)\n            cls_token, _, _ = self.backbone(crop)\n            # Pass the CLS token through the head\n            outputs.append(self.head(cls_token))\n\n        return outputs","metadata":{"id":"OvAJ9_dZ2y7m","trusted":true,"execution":{"iopub.status.busy":"2025-05-19T10:58:48.042362Z","iopub.execute_input":"2025-05-19T10:58:48.042540Z","iopub.status.idle":"2025-05-19T10:58:48.066464Z","shell.execute_reply.started":"2025-05-19T10:58:48.042526Z","shell.execute_reply":"2025-05-19T10:58:48.065949Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"def train_one_epoch(student: nn.Module, teacher: nn.Module, train_loader: DataLoader, dino_loss: DINOLoss,\n                   optimizer: optim.Optimizer, epoch: int, total_epochs: int, writer: SummaryWriter,\n                   warmup_teacher_temp_epochs: int = 5, teacher_temp: float = 0.04,\n                   momentum_schedule: np.ndarray = None, clip_grad: float = 0.):\n    \"\"\"\n    One epoch of DINOv2-like training.\n    \"\"\"\n    student.train() # Set student to training mode\n    teacher.eval()  # Teacher is in evaluation mode (no gradient updates)\n\n    metric_logger = MetricLogger()\n    header = f'Epoch: [{epoch}/{total_epochs}]'\n\n    # Adjust teacher temperature schedule\n    # This schedule is applied per epoch\n    teacher_temp_schedule = np.concatenate((\n        np.linspace(0.07, teacher_temp, warmup_teacher_temp_epochs),\n        np.ones(total_epochs - warmup_teacher_temp_epochs) * teacher_temp\n    ))\n    curr_teacher_temp = teacher_temp_schedule[epoch]\n\n\n    for it, (images, _) in enumerate(metric_logger.log_every(train_loader, 10, header)):\n        # Update weight decay and learning rate (if using a scheduler)\n        # This typically happens per iteration in DINO\n        it_global = len(train_loader) * epoch + it  # global iteration\n\n        # Example of updating LR (requires an LR scheduler)\n        # for i, param_group in enumerate(optimizer.param_groups):\n        #     param_group[\"lr\"] = lr_schedule[it_global] # Assuming lr_schedule is defined and accessible\n\n\n        # Move images (list of crops) to gpu\n        # Check if args.gpu is available and not -1 (for CPU)\n        device = next(student.parameters()).device # Get device from model parameters\n        images = [im.to(device, non_blocking=True) for im in images]\n\n\n        # Teacher and student forward passes\n        # Teacher only processes the two global views\n        teacher_output = teacher(images[:2])\n        # Student processes all views (global and local)\n        student_output = student(images)\n\n        # Loss computation\n        loss = dino_loss(student_output, teacher_output, curr_teacher_temp)\n\n        # Backward and optimize\n        optimizer.zero_grad()\n        loss.backward()\n\n        # Clip gradients (optional, but common in DINO)\n        if clip_grad > 0:\n             torch.nn.utils.clip_grad_norm_(student.parameters(), clip_grad)\n\n        optimizer.step()\n\n        # EMA update of the teacher\n        # Momentum is typically scheduled per iteration\n        with torch.no_grad():\n            m = momentum_schedule[it_global]  # momentum parameter\n            # Access the underlying module if using DDP\n            student_m = student.module if hasattr(student, 'module') else student\n            teacher_m = teacher.module if hasattr(teacher, 'module') else teacher\n            for param_q, param_k in zip(student_m.parameters(), teacher_m.parameters()):\n                param_k.data.mul_(m).add_((1 - m) * param_q.detach().data)\n\n        # Log metrics to MetricLogger\n        metric_logger.update(loss=loss.item())\n        metric_logger.update(teacher_temp=curr_teacher_temp)\n        metric_logger.update(momentum=m)\n        # Log learning rate if using a scheduler\n        # metric_logger.update(lr=optimizer.param_groups[0][\"lr\"])\n\n        # Log metrics to TensorBoard per step (optional, can be noisy)\n        # if writer and (it + 1) % 10 == 0: # Log every 10 batches\n        #     step = epoch * len(train_loader) + it\n        #     writer.add_scalar('DINO/Loss/Train_Step', loss.item(), step)\n        #     writer.add_scalar('DINO/Teacher_Temp/Train_Step', curr_teacher_temp, step)\n        #     writer.add_scalar('DINO/Momentum/Train_Step', m, step)\n            # if using LR scheduler: writer.add_scalar('DINO/LR/Train_Step', optimizer.param_groups[0][\"lr\"], step)\n\n\n    # Log epoch metrics to TensorBoard\n    if writer:\n        writer.add_scalar('DINO/Loss/Train_Epoch', metric_logger.loss.global_avg, epoch)\n        writer.add_scalar('DINO/Teacher_Temp/Train_Epoch', curr_teacher_temp, epoch)\n        writer.add_scalar('DINO/Momentum/Train_Epoch', momentum_schedule[it_global], epoch) # Log final momentum of the epoch\n        # if using LR scheduler: writer.add_scalar('DINO/LR/Train_Epoch', optimizer.param_groups[0][\"lr\"], epoch)\n\n\n    # Return the metric values averaged over the epoch\n    metric_logger.synchronize_between_processes()\n    # print(\"Averaged stats:\", metric_logger) # MetricLogger prints at the end of log_every\n    return {k: meter.global_avg for k, meter in metric_logger.meters.items()}\n\n","metadata":{"id":"ynOcsT3a2yx2","trusted":true,"execution":{"iopub.status.busy":"2025-05-19T10:58:48.067082Z","iopub.execute_input":"2025-05-19T10:58:48.067288Z","iopub.status.idle":"2025-05-19T10:58:48.089320Z","shell.execute_reply.started":"2025-05-19T10:58:48.067263Z","shell.execute_reply":"2025-05-19T10:58:48.088594Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"metadata_csv_path = './mmibc/combined_unimodal_metadata.csv' # UPDATE THIS PATH\noutput_dir = './dino_unimodal_pretraining_output' # Output directory for logs and checkpoints\nepochs = 20\nbatch_size = 16 # Batch size per GPU (adjust based on your hardware)\nglobal_crops_scale = (0.5, 1.0)\nlocal_crops_scale = (0.2, 0.5)\nlocal_crops_number = 8\nglobal_size = 224 # Image size for global crops (should match ViT input)\nlocal_size = 96 # Image size for local crops\nin_chans = 1 # Number of input channels (1 for grayscale)\nembed_dim = 768 # ViT embedding dimension\ndepth = 12 # Number of ViT blocks\nnum_heads = 12 # Number of attention heads\nmlp_ratio = 4.\nqkv_bias = True\ndrop_rate = 0.\nattn_drop_rate = 0.\ndrop_path_rate = 0.1\nlayer_scale_init_value = 1e-5\nteacher_temp = 0.04\nwarmup_teacher_temp_epochs = 5\nstudent_temp = 0.1\ncenter_momentum = 0.9\nlearning_rate = 5e-4\nweight_decay = 0.04\nwarmup_epochs = 10\nclip_grad = 0. # Gradient clipping value (0 for no clipping)\nnum_workers = 8 # Number of data loading workers (adjust based on your system)\nseed = 42\nsave_freq = 10 # Checkpoint saving frequency (epochs)","metadata":{"id":"4ize5N952yiZ","trusted":true,"execution":{"iopub.status.busy":"2025-05-19T10:58:48.090102Z","iopub.execute_input":"2025-05-19T10:58:48.090266Z","iopub.status.idle":"2025-05-19T10:58:48.112332Z","shell.execute_reply.started":"2025-05-19T10:58:48.090253Z","shell.execute_reply":"2025-05-19T10:58:48.111835Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"# --- Distributed Training Setup (Simplified for Notebook) ---\n# This section allows the code to run in both single GPU/CPU and distributed environments.\n# In a notebook, you typically run on a single GPU/CPU.\n# If running with torch.distributed.launch, the environment variables will be set.\n\nargs = argparse.Namespace( # Create a namespace object to mimic argparse args\n    metadata_path=metadata_csv_path,\n    output_dir=output_dir,\n    epochs=epochs,\n    batch_size=batch_size,\n    global_crops_scale=global_crops_scale,\n    local_crops_scale=local_crops_scale,\n    local_crops_number=local_crops_number,\n    global_size=global_size,\n    local_size=local_size,\n    in_chans=in_chans,\n    embed_dim=embed_dim,\n    depth=depth,\n    num_heads=num_heads,\n    mlp_ratio=mlp_ratio,\n    qkv_bias=qkv_bias,\n    drop_rate=drop_rate,\n    attn_drop_rate=attn_drop_rate,\n    drop_path_rate=drop_path_rate,\n    layer_scale_init_value=layer_scale_init_value,\n    teacher_temp=teacher_temp,\n    warmup_teacher_temp_epochs=warmup_teacher_temp_epochs,\n    student_temp=student_temp,\n    center_momentum=center_momentum,\n    learning_rate=learning_rate,\n    weight_decay=weight_decay,\n    warmup_epochs=warmup_epochs,\n    clip_grad=clip_grad,\n    num_workers=num_workers,\n    seed=seed,\n    save_freq=save_freq,\n    local_rank=0 # Default for single process\n)\n\n# Check if distributed environment variables are set\nif 'RANK' in os.environ and 'WORLD_SIZE' in os.environ:\n    args.rank = int(os.environ[\"RANK\"])\n    args.world_size = int(os.environ['WORLD_SIZE'])\n    args.gpu = int(os.environ['LOCAL_RANK'])\n    torch.cuda.set_device(args.gpu)\n    dist.init_process_group(backend=\"nccl\", init_method=\"env://\", world_size=args.world_size, rank=args.rank)\n    logger.info(f\"| distributed init: rank {args.rank}, world {args.world_size}, gpu {args.gpu}\")\n    torch.distributed.barrier() # Wait for all processes to synchronize\nelse:\n    logger.info(\"Not using distributed training. Running on a single GPU or CPU.\")\n    args.rank = 0\n    args.world_size = 1\n    args.gpu = 0 # Assuming GPU 0 if available\n    if torch.cuda.is_available():\n        torch.cuda.set_device(args.gpu)\n    else:\n        args.gpu = -1 # Indicate CPU usage\n\n\n# Ensure output directory exists (only on rank 0)\nif args.rank == 0:\n    Path(args.output_dir).mkdir(parents=True, exist_ok=True)\n    # Setup TensorBoard writer (only on rank 0)\n    writer = SummaryWriter(log_dir=os.path.join(args.output_dir, 'runs'))\n    logger.info(f\"TensorBoard logs will be saved to: {os.path.join(args.output_dir, 'runs')}\")\nelse:\n    writer = None # Only rank 0 writes to TensorBoard\n\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iy6GCskM2yaT","outputId":"43d2d0f2-b02e-4595-e307-6f6bdefe3d73","trusted":true,"execution":{"iopub.status.busy":"2025-05-19T10:58:48.113127Z","iopub.execute_input":"2025-05-19T10:58:48.113344Z","iopub.status.idle":"2025-05-19T10:58:48.136600Z","shell.execute_reply.started":"2025-05-19T10:58:48.113321Z","shell.execute_reply":"2025-05-19T10:58:48.136073Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"# --- Model Initialization ---\n# Create student and teacher backbones (ViTEncoders)\n# These encoders will be trained to handle unimodal data\nstudent_backbone = ViTEncoder(\n    img_size=args.global_size, # Use global crop size for backbone input size\n    in_chans=args.in_chans,\n    embed_dim=args.embed_dim,\n    depth=args.depth,\n    num_heads=args.num_heads,\n    mlp_ratio=args.mlp_ratio,\n    qkv_bias=args.qkv_bias,\n    drop_rate=args.drop_rate,\n    attn_drop_rate=args.attn_drop_rate,\n    drop_path_rate=args.drop_path_rate,\n    layer_scale_init_value=args.layer_scale_init_value\n)\n# Teacher is a copy of the student initially\nteacher_backbone = copy.deepcopy(student_backbone)\n\n# Create DINO heads for student and teacher\n# Output dimension (out_dim) is typically large (e.g., 65536)\ndino_out_dim = 65536 # Example output dimension\nstudent_head = DINOHead(in_dim=args.embed_dim, out_dim=dino_out_dim)\nteacher_head = DINOHead(in_dim=args.embed_dim, out_dim=dino_out_dim)\n\n# Wrap backbones and heads in MultiCropWrapper\nstudent = MultiCropWrapper(student_backbone, student_head)\nteacher = MultiCropWrapper(teacher_backbone, teacher_head)\n\n# Move models to GPU\nif args.gpu != -1:\n    student = student.cuda(args.gpu)\n    teacher = teacher.cuda(args.gpu)\nelse:\n    # Move to CPU if no GPU available\n    student = student.cpu()\n    teacher = teacher.cpu()\n\n\n# Freeze teacher parameters\nfor p in teacher.parameters():\n    p.requires_grad = False\n\n# Wrap student model with DistributedDataParallel if using distributed training\nif dist.is_initialized():\n    student = torch.nn.parallel.DistributedDataParallel(student, device_ids=[args.gpu])\n    # Teacher does not need DDP as it's updated via EMA\n","metadata":{"id":"2SIj-CJu0Nrg","trusted":true,"execution":{"iopub.status.busy":"2025-05-19T10:58:48.137312Z","iopub.execute_input":"2025-05-19T10:58:48.137685Z","iopub.status.idle":"2025-05-19T10:58:50.238829Z","shell.execute_reply.started":"2025-05-19T10:58:48.137664Z","shell.execute_reply":"2025-05-19T10:58:50.237962Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"# Sampler for distributed training\n# Shuffle is handled by the sampler in distributed mode\nsampler = torch.utils.data.distributed.DistributedSampler(unimodal_dataset, shuffle=True) if dist.is_initialized() else None\n\n# DataLoader\ndata_loader = DataLoader(\n    unimodal_dataset,\n    sampler=sampler, # Use sampler in distributed mode\n    batch_size=args.batch_size,\n    shuffle=(sampler is None), # Shuffle only if not using distributed sampler\n    num_workers=args.num_workers,\n    pin_memory=(args.gpu != -1), # Pin memory only if using GPU\n    drop_last=True, # Drop the last incomplete batch\n)\nlogger.info(f\"DataLoader created with batch size {args.batch_size} and {len(data_loader)} batches per epoch.\")\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Sbkb4ii67Zsl","outputId":"7ce8baf4-c620-4d1d-dfc3-cbaa74ee45d1","trusted":true,"execution":{"iopub.status.busy":"2025-05-19T10:58:50.239793Z","iopub.execute_input":"2025-05-19T10:58:50.240102Z","iopub.status.idle":"2025-05-19T10:58:50.246002Z","shell.execute_reply.started":"2025-05-19T10:58:50.240078Z","shell.execute_reply":"2025-05-19T10:58:50.245194Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"# --- Loss Function and Optimizer ---\ndino_loss = DINOLoss(\n    out_dim=dino_out_dim,\n    teacher_temp=args.teacher_temp,\n    student_temp=args.student_temp,\n    center_momentum=args.center_momentum\n)\nif args.gpu != -1:\n    dino_loss = dino_loss.cuda(args.gpu) # Move loss to GPU\nelse:\n    dino_loss = dino_loss.cpu() # Move loss to CPU\n\n\n# Optimizer\n# Parameters to optimize: student backbone and student head\noptimizer = optim.AdamW(student.parameters(), lr=args.learning_rate, weight_decay=args.weight_decay)\n\n# --- Schedulers ---\n# Learning rate scheduler\nlr_schedule = cosine_scheduler(\n    base_value=args.learning_rate * args.world_size * args.batch_size / 256., # Scale LR linearly with batch size (common practice)\n    final_value=args.learning_rate * 1e-6, # Example: decay to 1e-6 of base\n    epochs=args.epochs,\n    niter_per_ep=len(data_loader),\n    warmup_epochs=args.warmup_epochs,\n)\n\n# Momentum scheduler for teacher EMA update\nmomentum_schedule = cosine_scheduler(\n    base_value=0.996, # Start momentum\n    final_value=1.0,  # End momentum\n    epochs=args.epochs,\n    niter_per_ep=len(data_loader),\n)\n","metadata":{"id":"0i3FP2M37eLB","trusted":true,"execution":{"iopub.status.busy":"2025-05-19T10:58:50.246919Z","iopub.execute_input":"2025-05-19T10:58:50.247178Z","iopub.status.idle":"2025-05-19T10:58:50.273888Z","shell.execute_reply.started":"2025-05-19T10:58:50.247156Z","shell.execute_reply":"2025-05-19T10:58:50.273181Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"# prompt: #print used GPU Memory\n\n# Check if CUDA is available\nif torch.cuda.is_available():\n    print(\"CUDA is available. Using GPU:\")\n    # Get the current GPU device\n    device = torch.cuda.current_device()\n    print(f\"Device Name: {torch.cuda.get_device_name(device)}\")\n    print(f\"Total Memory: {torch.cuda.get_device_properties(device).total_memory / 1024**3:.2f} GB\")\n\n    # Get used and free memory\n    # This requires synchronizing\n    torch.cuda.empty_cache() # Clear cache to get more accurate free memory\n    allocated_memory = torch.cuda.memory_allocated(device)\n    cached_memory = torch.cuda.memory_reserved(device)\n    print(f\"Allocated Memory: {allocated_memory / 1024**3:.2f} GB\")\n    print(f\"Cached Memory:    {cached_memory / 1024**3:.2f} GB\")\n\n    # This part is more complex to get *only* memory used by your *current* process\n    # without external tools or more specific CUDA APIs.\n    # The allocated_memory gives memory currently allocated by PyTorch.\n\nelse:\n    print(\"CUDA is not available. Running on CPU.\")\n\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"92gpQNOo9fxm","outputId":"6bc4e315-bc48-4463-f4d1-2fb6676431c3","trusted":true,"execution":{"iopub.status.busy":"2025-05-19T10:58:50.274687Z","iopub.execute_input":"2025-05-19T10:58:50.274906Z","iopub.status.idle":"2025-05-19T10:58:50.288723Z","shell.execute_reply.started":"2025-05-19T10:58:50.274882Z","shell.execute_reply":"2025-05-19T10:58:50.288132Z"}},"outputs":[{"name":"stdout","text":"CUDA is available. Using GPU:\nDevice Name: Tesla T4\nTotal Memory: 14.74 GB\nAllocated Memory: 0.81 GB\nCached Memory:    0.88 GB\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"# prompt: clear allocated memory completely to 0GB\n\nimport gc\nimport torch\n\ngc.collect()\nif torch.cuda.is_available():\n  torch.cuda.empty_cache()\n\nprint(\"Memory cleared.\")\nif torch.cuda.is_available():\n  device = torch.cuda.current_device()\n  allocated_memory = torch.cuda.memory_allocated(device)\n  cached_memory = torch.cuda.memory_reserved(device)\n  print(f\"Allocated Memory: {allocated_memory / 1024**3:.2f} GB\")\n  print(f\"Cached Memory:    {cached_memory / 1024**3:.2f} GB\")\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Sd7E1wA9_B8Y","outputId":"0239e655-05e9-4dd8-f49b-74e36dab1c26","trusted":true,"execution":{"iopub.status.busy":"2025-05-19T10:58:50.289374Z","iopub.execute_input":"2025-05-19T10:58:50.289602Z","iopub.status.idle":"2025-05-19T10:58:50.605745Z","shell.execute_reply.started":"2025-05-19T10:58:50.289579Z","shell.execute_reply":"2025-05-19T10:58:50.604952Z"}},"outputs":[{"name":"stdout","text":"Memory cleared.\nAllocated Memory: 0.81 GB\nCached Memory:    0.88 GB\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"# --- Training Loop ---\nlogger.info(f\"Starting DINO unimodal pre-training for {args.epochs} epochs...\")\n\nfor epoch in range(args.epochs):\n    # Set sampler epoch for distributed training\n    if dist.is_initialized():\n        data_loader.sampler.set_epoch(epoch)\n\n    # Train one epoch\n    # Pass the current learning rate from the schedule to the optimizer\n    for i, param_group in enumerate(optimizer.param_groups):\n         param_group[\"lr\"] = lr_schedule[epoch * len(data_loader) + i]\n\n\n    train_stats = train_one_epoch(\n        student=student,\n        teacher=teacher,\n        train_loader=data_loader,\n        dino_loss=dino_loss,\n        optimizer=optimizer,\n        epoch=epoch,\n        total_epochs=args.epochs,\n        writer=writer,\n        warmup_teacher_temp_epochs=args.warmup_teacher_temp_epochs,\n        teacher_temp=args.teacher_temp, # Pass base teacher temp\n        momentum_schedule=momentum_schedule,\n        clip_grad=args.clip_grad\n    )\n\n    # Save checkpoint (only on rank 0)\n    if args.rank == 0 and (epoch + 1) % args.save_freq == 0:\n         # Save student backbone state dict\n         # If using DDP, access the underlying module\n         student_backbone_to_save = student.module.backbone if hasattr(student, 'module') else student.backbone\n         save_checkpoint({\n             'epoch': epoch + 1,\n             'student_backbone_state_dict': student_backbone_to_save.state_dict(),\n             'optimizer_state_dict': optimizer.state_dict(),\n             'args': args, # Save training arguments\n             'train_stats': train_stats # Save epoch stats\n         }, is_best=False, filename=f'dino_checkpoint_epoch_{epoch+1}.pth', save_dir=args.output_dir)\n\n\n# Save final student backbone weights (only on rank 0)\nif args.rank == 0:\n    student_backbone_to_save = student.module.backbone if hasattr(student, 'module') else student.backbone\n    torch.save(student_backbone_to_save.state_dict(), os.path.join(args.output_dir, 'dinov2_unimodal_backbone.pth'))\n    logger.info(f\"Final pretrained unimodal student backbone saved to {os.path.join(args.output_dir, 'dinov2_unimodal_backbone.pth')}\")\n\n\n# Close TensorBoard writer (only on rank 0)\nif args.rank == 0 and writer:\n    writer.close()\n\n# Clean up distributed training (important when running in script mode, less critical in notebook unless explicitly using distributed)\n# if dist.is_initialized():\n#     dist.destroy_process_group()\n\n","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":495},"id":"km2gUmhu5pRG","outputId":"1bf38f6e-cf44-4459-94f2-37f2839f752f","trusted":true,"execution":{"iopub.status.busy":"2025-05-19T10:58:50.606558Z","iopub.execute_input":"2025-05-19T10:58:50.606781Z"}},"outputs":[{"name":"stdout","text":"Epoch: [0/20]\nEpoch: [0/20] [0/1306]\teta: 1:33:59\ttime: 6.6577\tdata: 1.9790\tloss: 11.0904 (11.0904)\tteacher_temp: 0.0700 (0.0700)\tmomentum: 0.9960 (0.9960)\nEpoch: [0/20] [10/1306]\teta: 0:24:46\ttime: 2.2941\tdata: 0.0003\tloss: 11.0904 (11.0904)\tteacher_temp: 0.0700 (0.0700)\tmomentum: 0.9960 (0.9960)\nEpoch: [0/20] [20/1306]\teta: 0:23:29\ttime: 2.1917\tdata: 0.0002\tloss: 10.5622 (11.0904)\tteacher_temp: 0.0667 (0.0700)\tmomentum: 0.9486 (0.9960)\nEpoch: [0/20] [30/1306]\teta: 0:22:49\ttime: 2.1467\tdata: 0.0002\tloss: 7.1551 (11.0904)\tteacher_temp: 0.0452 (0.0700)\tmomentum: 0.6426 (0.9960)\nEpoch: [0/20] [40/1306]\teta: 0:22:52\ttime: 2.1680\tdata: 0.0002\tloss: 5.4099 (11.0904)\tteacher_temp: 0.0341 (0.0700)\tmomentum: 0.4859 (0.9960)\nEpoch: [0/20] [50/1306]\teta: 0:22:59\ttime: 2.1958\tdata: 0.0002\tloss: 4.3492 (11.0904)\tteacher_temp: 0.0275 (0.0700)\tmomentum: 0.3906 (0.9960)\nEpoch: [0/20] [60/1306]\teta: 0:22:40\ttime: 2.1843\tdata: 0.0002\tloss: 3.6362 (11.0904)\tteacher_temp: 0.0230 (0.0700)\tmomentum: 0.3266 (0.9960)\nEpoch: [0/20] [70/1306]\teta: 0:22:26\ttime: 2.1783\tdata: 0.0003\tloss: 3.1240 (11.0904)\tteacher_temp: 0.0197 (0.0700)\tmomentum: 0.2806 (0.9960)\nEpoch: [0/20] [80/1306]\teta: 0:22:12\ttime: 2.1742\tdata: 0.0002\tloss: 2.7384 (11.0904)\tteacher_temp: 0.0173 (0.0700)\tmomentum: 0.2459 (0.9960)\nEpoch: [0/20] [90/1306]\teta: 0:22:03\ttime: 2.1768\tdata: 0.0002\tloss: 2.4374 (11.0904)\tteacher_temp: 0.0154 (0.0700)\tmomentum: 0.2189 (0.9960)\nEpoch: [0/20] [100/1306]\teta: 0:21:55\ttime: 2.1814\tdata: 0.0002\tloss: 2.1961 (11.0904)\tteacher_temp: 0.0139 (0.0700)\tmomentum: 0.1972 (0.9960)\nEpoch: [0/20] [110/1306]\teta: 0:21:50\ttime: 2.1918\tdata: 0.0002\tloss: 1.9983 (11.0904)\tteacher_temp: 0.0126 (0.0700)\tmomentum: 0.1795 (0.9960)\nEpoch: [0/20] [120/1306]\teta: 0:21:36\ttime: 2.1864\tdata: 0.0002\tloss: 1.8331 (11.0904)\tteacher_temp: 0.0116 (0.0700)\tmomentum: 0.1646 (0.9960)\nEpoch: [0/20] [130/1306]\teta: 0:21:24\ttime: 2.1840\tdata: 0.0002\tloss: 1.6932 (11.0904)\tteacher_temp: 0.0107 (0.0700)\tmomentum: 0.1521 (0.9960)\nEpoch: [0/20] [140/1306]\teta: 0:21:11\ttime: 2.1799\tdata: 0.0003\tloss: 1.5731 (11.0904)\tteacher_temp: 0.0099 (0.0700)\tmomentum: 0.1413 (0.9960)\nEpoch: [0/20] [150/1306]\teta: 0:20:59\ttime: 2.1793\tdata: 0.0004\tloss: 1.4689 (11.0904)\tteacher_temp: 0.0093 (0.0700)\tmomentum: 0.1319 (0.9960)\nEpoch: [0/20] [160/1306]\teta: 0:20:47\ttime: 2.1766\tdata: 0.0002\tloss: 1.3777 (11.0904)\tteacher_temp: 0.0087 (0.0700)\tmomentum: 0.1237 (0.9960)\nEpoch: [0/20] [170/1306]\teta: 0:20:35\ttime: 2.1750\tdata: 0.0002\tloss: 1.2971 (11.0904)\tteacher_temp: 0.0082 (0.0700)\tmomentum: 0.1165 (0.9960)\nEpoch: [0/20] [180/1306]\teta: 0:20:26\ttime: 2.1784\tdata: 0.0002\tloss: 1.2255 (11.0904)\tteacher_temp: 0.0077 (0.0700)\tmomentum: 0.1101 (0.9960)\nEpoch: [0/20] [190/1306]\teta: 0:20:12\ttime: 2.1726\tdata: 0.0002\tloss: 1.1613 (11.0904)\tteacher_temp: 0.0073 (0.0700)\tmomentum: 0.1043 (0.9960)\nEpoch: [0/20] [200/1306]\teta: 0:20:04\ttime: 2.1781\tdata: 0.0002\tloss: 1.1035 (11.0904)\tteacher_temp: 0.0070 (0.0700)\tmomentum: 0.0991 (0.9960)\nEpoch: [0/20] [210/1306]\teta: 0:19:53\ttime: 2.1781\tdata: 0.0002\tloss: 1.0512 (11.0904)\tteacher_temp: 0.0066 (0.0700)\tmomentum: 0.0944 (0.9960)\nEpoch: [0/20] [220/1306]\teta: 0:19:43\ttime: 2.1792\tdata: 0.0002\tloss: 1.0037 (11.0904)\tteacher_temp: 0.0063 (0.0700)\tmomentum: 0.0901 (0.9960)\nEpoch: [0/20] [230/1306]\teta: 0:19:30\ttime: 2.1756\tdata: 0.0002\tloss: 0.9602 (11.0904)\tteacher_temp: 0.0061 (0.0700)\tmomentum: 0.0862 (0.9960)\nEpoch: [0/20] [240/1306]\teta: 0:19:20\ttime: 2.1775\tdata: 0.0002\tloss: 0.9204 (11.0904)\tteacher_temp: 0.0058 (0.0700)\tmomentum: 0.0827 (0.9960)\nEpoch: [0/20] [250/1306]\teta: 0:19:10\ttime: 2.1780\tdata: 0.0002\tloss: 0.8837 (11.0904)\tteacher_temp: 0.0056 (0.0700)\tmomentum: 0.0794 (0.9960)\nEpoch: [0/20] [260/1306]\teta: 0:19:00\ttime: 2.1805\tdata: 0.0002\tloss: 0.8498 (11.0904)\tteacher_temp: 0.0054 (0.0700)\tmomentum: 0.0763 (0.9960)\nEpoch: [0/20] [270/1306]\teta: 0:18:45\ttime: 2.1728\tdata: 0.0002\tloss: 0.8185 (11.0904)\tteacher_temp: 0.0052 (0.0700)\tmomentum: 0.0735 (0.9960)\nEpoch: [0/20] [280/1306]\teta: 0:18:38\ttime: 2.1810\tdata: 0.0002\tloss: 0.7893 (11.0904)\tteacher_temp: 0.0050 (0.0700)\tmomentum: 0.0709 (0.9960)\nEpoch: [0/20] [290/1306]\teta: 0:18:25\ttime: 2.1766\tdata: 0.0002\tloss: 0.7622 (11.0904)\tteacher_temp: 0.0048 (0.0700)\tmomentum: 0.0685 (0.9960)\nEpoch: [0/20] [300/1306]\teta: 0:18:16\ttime: 2.1795\tdata: 0.0002\tloss: 0.7369 (11.0904)\tteacher_temp: 0.0047 (0.0700)\tmomentum: 0.0662 (0.9960)\nEpoch: [0/20] [310/1306]\teta: 0:18:06\ttime: 2.1808\tdata: 0.0002\tloss: 0.7132 (11.0904)\tteacher_temp: 0.0045 (0.0700)\tmomentum: 0.0641 (0.9960)\nEpoch: [0/20] [320/1306]\teta: 0:17:55\ttime: 2.1808\tdata: 0.0001\tloss: 0.6910 (11.0904)\tteacher_temp: 0.0044 (0.0700)\tmomentum: 0.0621 (0.9960)\nEpoch: [0/20] [330/1306]\teta: 0:17:44\ttime: 2.1805\tdata: 0.0003\tloss: 0.6701 (11.0904)\tteacher_temp: 0.0042 (0.0700)\tmomentum: 0.0602 (0.9960)\nEpoch: [0/20] [340/1306]\teta: 0:17:33\ttime: 2.1815\tdata: 0.0002\tloss: 0.6505 (11.0904)\tteacher_temp: 0.0041 (0.0700)\tmomentum: 0.0584 (0.9960)\nEpoch: [0/20] [350/1306]\teta: 0:17:24\ttime: 2.1845\tdata: 0.0002\tloss: 0.6319 (11.0904)\tteacher_temp: 0.0040 (0.0700)\tmomentum: 0.0568 (0.9960)\nEpoch: [0/20] [360/1306]\teta: 0:17:11\ttime: 2.1811\tdata: 0.0002\tloss: 0.6144 (11.0904)\tteacher_temp: 0.0039 (0.0700)\tmomentum: 0.0552 (0.9960)\nEpoch: [0/20] [370/1306]\teta: 0:17:02\ttime: 2.1836\tdata: 0.0002\tloss: 0.5979 (11.0904)\tteacher_temp: 0.0038 (0.0700)\tmomentum: 0.0537 (0.9960)\nEpoch: [0/20] [380/1306]\teta: 0:16:52\ttime: 2.1868\tdata: 0.0002\tloss: 0.5822 (11.0904)\tteacher_temp: 0.0037 (0.0700)\tmomentum: 0.0523 (0.9960)\nEpoch: [0/20] [390/1306]\teta: 0:16:41\ttime: 2.1874\tdata: 0.0002\tloss: 0.5673 (11.0904)\tteacher_temp: 0.0036 (0.0700)\tmomentum: 0.0509 (0.9960)\nEpoch: [0/20] [400/1306]\teta: 0:16:28\ttime: 2.1814\tdata: 0.0002\tloss: 0.5531 (11.0904)\tteacher_temp: 0.0035 (0.0700)\tmomentum: 0.0497 (0.9960)\nEpoch: [0/20] [410/1306]\teta: 0:16:17\ttime: 2.1828\tdata: 0.0002\tloss: 0.5397 (11.0904)\tteacher_temp: 0.0034 (0.0700)\tmomentum: 0.0485 (0.9960)\nEpoch: [0/20] [420/1306]\teta: 0:16:06\ttime: 2.1811\tdata: 0.0002\tloss: 0.5269 (11.0904)\tteacher_temp: 0.0033 (0.0700)\tmomentum: 0.0473 (0.9960)\nEpoch: [0/20] [430/1306]\teta: 0:15:53\ttime: 2.1776\tdata: 0.0001\tloss: 0.5146 (11.0904)\tteacher_temp: 0.0032 (0.0700)\tmomentum: 0.0462 (0.9960)\nEpoch: [0/20] [440/1306]\teta: 0:15:43\ttime: 2.1777\tdata: 0.0002\tloss: 0.5030 (11.0904)\tteacher_temp: 0.0032 (0.0700)\tmomentum: 0.0452 (0.9960)\nEpoch: [0/20] [450/1306]\teta: 0:15:32\ttime: 2.1783\tdata: 0.0002\tloss: 0.4918 (11.0904)\tteacher_temp: 0.0031 (0.0700)\tmomentum: 0.0442 (0.9960)\nEpoch: [0/20] [460/1306]\teta: 0:15:23\ttime: 2.1829\tdata: 0.0002\tloss: 0.4811 (11.0904)\tteacher_temp: 0.0030 (0.0700)\tmomentum: 0.0432 (0.9960)\nEpoch: [0/20] [470/1306]\teta: 0:15:12\ttime: 2.1838\tdata: 0.0002\tloss: 0.4709 (11.0904)\tteacher_temp: 0.0030 (0.0700)\tmomentum: 0.0423 (0.9960)\nEpoch: [0/20] [480/1306]\teta: 0:14:59\ttime: 2.1772\tdata: 0.0002\tloss: 0.4611 (11.0904)\tteacher_temp: 0.0029 (0.0700)\tmomentum: 0.0414 (0.9960)\nEpoch: [0/20] [490/1306]\teta: 0:14:49\ttime: 2.1803\tdata: 0.0002\tloss: 0.4517 (11.0904)\tteacher_temp: 0.0029 (0.0700)\tmomentum: 0.0406 (0.9960)\nEpoch: [0/20] [500/1306]\teta: 0:14:40\ttime: 2.1850\tdata: 0.0002\tloss: 0.4427 (11.0904)\tteacher_temp: 0.0028 (0.0700)\tmomentum: 0.0398 (0.9960)\nEpoch: [0/20] [510/1306]\teta: 0:14:27\ttime: 2.1799\tdata: 0.0002\tloss: 0.4341 (11.0904)\tteacher_temp: 0.0027 (0.0700)\tmomentum: 0.0390 (0.9960)\nEpoch: [0/20] [520/1306]\teta: 0:14:17\ttime: 2.1807\tdata: 0.0003\tloss: 0.4257 (11.0904)\tteacher_temp: 0.0027 (0.0700)\tmomentum: 0.0382 (0.9960)\nEpoch: [0/20] [530/1306]\teta: 0:14:07\ttime: 2.1844\tdata: 0.0002\tloss: 0.4177 (11.0904)\tteacher_temp: 0.0026 (0.0700)\tmomentum: 0.0375 (0.9960)\nEpoch: [0/20] [540/1306]\teta: 0:13:55\ttime: 2.1823\tdata: 0.0002\tloss: 0.4100 (11.0904)\tteacher_temp: 0.0026 (0.0700)\tmomentum: 0.0368 (0.9960)\nEpoch: [0/20] [550/1306]\teta: 0:13:43\ttime: 2.1796\tdata: 0.0002\tloss: 0.4026 (11.0904)\tteacher_temp: 0.0025 (0.0700)\tmomentum: 0.0362 (0.9960)\nEpoch: [0/20] [560/1306]\teta: 0:13:35\ttime: 2.1849\tdata: 0.0003\tloss: 0.3954 (11.0904)\tteacher_temp: 0.0025 (0.0700)\tmomentum: 0.0355 (0.9960)\nEpoch: [0/20] [570/1306]\teta: 0:13:23\ttime: 2.1818\tdata: 0.0002\tloss: 0.3885 (11.0904)\tteacher_temp: 0.0025 (0.0700)\tmomentum: 0.0349 (0.9960)\nEpoch: [0/20] [580/1306]\teta: 0:13:14\ttime: 2.1885\tdata: 0.0002\tloss: 0.3818 (11.0904)\tteacher_temp: 0.0024 (0.0700)\tmomentum: 0.0343 (0.9960)\nEpoch: [0/20] [590/1306]\teta: 0:13:03\ttime: 2.1892\tdata: 0.0002\tloss: 0.3753 (11.0904)\tteacher_temp: 0.0024 (0.0700)\tmomentum: 0.0337 (0.9960)\nEpoch: [0/20] [600/1306]\teta: 0:12:50\ttime: 2.1837\tdata: 0.0002\tloss: 0.3691 (11.0904)\tteacher_temp: 0.0023 (0.0700)\tmomentum: 0.0331 (0.9960)\nEpoch: [0/20] [610/1306]\teta: 0:12:38\ttime: 2.1786\tdata: 0.0003\tloss: 0.3630 (11.0904)\tteacher_temp: 0.0023 (0.0700)\tmomentum: 0.0326 (0.9960)\nEpoch: [0/20] [620/1306]\teta: 0:12:29\ttime: 2.1844\tdata: 0.0004\tloss: 0.3572 (11.0904)\tteacher_temp: 0.0023 (0.0700)\tmomentum: 0.0321 (0.9960)\nEpoch: [0/20] [630/1306]\teta: 0:12:16\ttime: 2.1783\tdata: 0.0002\tloss: 0.3515 (11.0904)\tteacher_temp: 0.0022 (0.0700)\tmomentum: 0.0316 (0.9960)\nEpoch: [0/20] [640/1306]\teta: 0:12:04\ttime: 2.1742\tdata: 0.0002\tloss: 0.3460 (11.0904)\tteacher_temp: 0.0022 (0.0700)\tmomentum: 0.0311 (0.9960)\nEpoch: [0/20] [650/1306]\teta: 0:11:53\ttime: 2.1743\tdata: 0.0003\tloss: 0.3407 (11.0904)\tteacher_temp: 0.0022 (0.0700)\tmomentum: 0.0306 (0.9960)\nEpoch: [0/20] [660/1306]\teta: 0:11:43\ttime: 2.1786\tdata: 0.0002\tloss: 0.3356 (11.0904)\tteacher_temp: 0.0021 (0.0700)\tmomentum: 0.0301 (0.9960)\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"id":"ImBAS6q87nQ_","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}